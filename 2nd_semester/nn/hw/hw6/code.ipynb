{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qSEOC5xB_GH"
      },
      "source": [
        "<div align=\"center\"><h1>HW6</h1></div>\n",
        "<div align=\"center\"><h2>Mohammadreza Ghofrani, 400131076</h2></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9NwQAD4B_GL"
      },
      "source": [
        "# Dependancies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "LegQIXe3B_GM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import backend\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "6iCv8RrgCQ1_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading"
      ],
      "metadata": {
        "id": "x1pDDFQfTGlb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1SLrmOfW66WQc-zrfo-gX-g2dDA4pAw1F\n",
        "!unzip -o NN_HW6_Dataset.zip -d data/\n",
        "!rm 'data/IRX6XTPI0009[2022-05-23-10-18-57].csv'"
      ],
      "metadata": {
        "id": "MOm38lFVRj-t",
        "outputId": "c4e9ada4-14bd-4f08-9947-c8f1ed5c48e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1SLrmOfW66WQc-zrfo-gX-g2dDA4pAw1F\n",
            "To: /content/NN_HW6_Dataset.zip\n",
            "\r  0% 0.00/1.80M [00:00<?, ?B/s]\r100% 1.80M/1.80M [00:00<00:00, 129MB/s]\n",
            "Archive:  NN_HW6_Dataset.zip\n",
            "  inflating: data/IRX6XAFF0005.csv   \n",
            "  inflating: data/IRX6XALS0002.csv   \n",
            "  inflating: data/IRX6XS300003.csv   \n",
            "  inflating: data/IRX6XSLC0000.csv   \n",
            "  inflating: data/IRX6XSNT0009.csv   \n",
            "  inflating: data/IRX6XTAL0001.csv   \n",
            "  inflating: data/IRX6XTDP0004.csv   \n",
            "  inflating: data/IRX6XTPI0009.csv   \n",
            "  inflating: data/IRX6XTPI0009[2022-05-23-10-18-57].csv  \n",
            "  inflating: data/IRX6XTPI0025.csv   \n",
            "  inflating: data/IRX6XWAI0001.csv   \n",
            "  inflating: data/IRX6XWTH0001.csv   \n",
            "  inflating: data/IRXWXEXR0007.csv   \n",
            "  inflating: data/IRXWXEXR0023.csv   \n",
            "  inflating: data/IRXWXOCI0001.csv   \n",
            "  inflating: data/IRXWXOCI0027.csv   \n",
            "  inflating: data/IRXYXTPI0009.csv   \n",
            "  inflating: data/IRXYXTPI0025.csv   \n",
            "  inflating: data/IRXZXAGR0009.csv   \n",
            "  inflating: data/IRXZXBNK0006.csv   \n",
            "  inflating: data/IRXZXCMI0000.csv   \n",
            "  inflating: data/IRXZXCML0005.csv   \n",
            "  inflating: data/IRXZXCNS0007.csv   \n",
            "  inflating: data/IRXZXENG0009.csv   \n",
            "  inflating: data/IRXZXEXR0004.csv   \n",
            "  inflating: data/IRXZXEXR0020.csv   \n",
            "  inflating: data/IRXZXFIN0006.csv   \n",
            "  inflating: data/IRXZXFOD0000.csv   \n",
            "  inflating: data/IRXZXHTL0003.csv   \n",
            "  inflating: data/IRXZXIND0008.csv   \n",
            "  inflating: data/IRXZXINS0001.csv   \n",
            "  inflating: data/IRXZXINV0006.csv   \n",
            "  inflating: data/IRXZXITG0009.csv   \n",
            "  inflating: data/IRXZXLSG0005.csv   \n",
            "  inflating: data/IRXZXMIN0007.csv   \n",
            "  inflating: data/IRXZXMML0003.csv   \n",
            "  inflating: data/IRXZXMNF0000.csv   \n",
            "  inflating: data/IRXZXMOT0003.csv   \n",
            "  inflating: data/IRXZXMTL0006.csv   \n",
            "  inflating: data/IRXZXNMM0001.csv   \n",
            "  inflating: data/IRXZXOBM0003.csv   \n",
            "  inflating: data/IRXZXOCI0008.csv   \n",
            "  inflating: data/IRXZXOCI0024.csv   \n",
            "  inflating: data/IRXZXOEW0008.csv   \n",
            "  inflating: data/IRXZXOIL0007.csv   \n",
            "  inflating: data/IRXZXOUT0003.csv   \n",
            "  inflating: data/IRXZXPHM0006.csv   \n",
            "  inflating: data/IRXZXRTL0001.csv   \n",
            "  inflating: data/IRXZXSRV0000.csv   \n",
            "  inflating: data/IRXZXTEX0002.csv   \n",
            "  inflating: data/IRXZXTRI0006.csv   \n",
            "  inflating: data/IRXZXWDI0007.csv   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read files"
      ],
      "metadata": {
        "id": "UtH7_aIKTJ_C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_directory = 'data'\n",
        "target_file = 'IRX6XTPI0009.csv'\n",
        "excluded_feature_file = 'IRXZXOBM0003.csv'\n",
        "dataframes = dict()\n",
        "for item in os.listdir(data_directory):\n",
        "    item_path = os.path.join(data_directory, item)\n",
        "    df = pd.read_csv(item_path, encoding='utf-16')\n",
        "    if not df.empty and not item == excluded_feature_file:\n",
        "        dataframes[item] = df"
      ],
      "metadata": {
        "id": "HXUXxmCySAVy"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess"
      ],
      "metadata": {
        "id": "DrVSmCT2Rle0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_df(df):\n",
        "    outdf = df.copy()\n",
        "    for col in outdf.columns:\n",
        "        if len(outdf[col].unique()) <= 2:\n",
        "            outdf.drop(col, axis=1, inplace=True)\n",
        "    return outdf"
      ],
      "metadata": {
        "id": "VWTjeF_vH7YA"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_names = list(dataframes.keys())\n",
        "\n",
        "merged_df = pd.DataFrame()\n",
        "for n in df_names:\n",
        "    df = dataframes[n]\n",
        "    df = preprocess_df(df)\n",
        "    if merged_df.empty:\n",
        "        merged_df = df\n",
        "    else:\n",
        "        merged_df = pd.merge(merged_df, df, how='outer', on='<DTYYYYMMDD>', suffixes=('', f'_{n}'))\n",
        "        \n",
        "merged_df = merged_df[(merged_df['<DTYYYYMMDD>'] > 20210523) & (merged_df['<DTYYYYMMDD>'] < 20220522)]\n",
        "merged_df.reset_index(inplace=True, drop=True)"
      ],
      "metadata": {
        "id": "t8JHnP6IyisA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label = []\n",
        "close_today = 0\n",
        "close_yesterday = 0\n",
        "\n",
        "for row in merged_df.iterrows():\n",
        "    idx, seri = row\n",
        "\n",
        "    if idx == 0:\n",
        "        close_yesterday = seri[f'<CLOSE>_{target_file}']\n",
        "        continue\n",
        "\n",
        "    close_today = seri[f'<CLOSE>_{target_file}']\n",
        "    l = np.ceil((np.sign(close_today - close_yesterday) + 1) / 2)\n",
        "    close_yesterday = close_today\n",
        "    label.append(l)\n",
        "\n",
        "merged_df['label'] = np.pad(label, (0,1), mode='constant',constant_values=(np.nan,))"
      ],
      "metadata": {
        "id": "6B2aVa1wRjbK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tmp = merged_df.sort_values(by=['<DTYYYYMMDD>'])\n",
        "\n",
        "tmp.reset_index(drop=True, inplace=True)\n",
        "for icol, col in enumerate(tmp.columns):\n",
        "    nan_indices = tmp[tmp[col].isnull()].index.tolist()\n",
        "    if col == 'label':\n",
        "        tmp.iloc[irow, icol] = 1\n",
        "    else:\n",
        "        for irow in nan_indices:\n",
        "            tmp.iloc[irow, icol] = (tmp.iloc[irow+1, icol] + tmp.iloc[irow-1, icol])/2\n",
        "\n",
        "tmp = tmp.drop(labels=[col for col in tmp if col.endswith(target_file)], axis=1)\n",
        "tmp = tmp.drop(labels=['<DTYYYYMMDD>'], axis=1)\n",
        "tmp.reset_index(drop=True, inplace=True)\n",
        "tmp.drop(tmp.tail(1).index, inplace=True) # drop last n rows\n",
        "xdf, ydf = tmp, tmp['label']"
      ],
      "metadata": {
        "id": "WE7-MfEtEEXT"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = MinMaxScaler()\n",
        "xdf = scaler.fit_transform(xdf.to_numpy())"
      ],
      "metadata": {
        "id": "sNC15jDzcvHc"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "step = 10\n",
        "n_feature = xdf.shape[1]\n",
        "n_data = len(ydf) - step\n",
        "X, y = np.zeros((n_data, step, n_feature)), np.zeros((n_data,))\n",
        "for i in range(n_data):\n",
        "    X[i]  = xdf[i:i+step]\n",
        "    y[i] = ydf[i+step]"
      ],
      "metadata": {
        "id": "1vgpmTIhRahG"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Splitting into sets"
      ],
      "metadata": {
        "id": "q26Lw0F5TX96"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xtrain, xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, random_state=46)\n",
        "xtrain, xval, ytrain, yval = train_test_split(xtrain, ytrain, test_size=0.125, random_state=121)"
      ],
      "metadata": {
        "id": "V-2B3h2BIU9-"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 2"
      ],
      "metadata": {
        "id": "qZyy3tHJS-AP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Baseline"
      ],
      "metadata": {
        "id": "YardtkYjb5hG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_train_acc = np.sum(ytrain[ytrain==1])/len(ytrain)\n",
        "baseline_val_acc = np.sum(yval[yval==1])/len(yval)\n",
        "baseline_test_acc = np.sum(ytest[ytest==1])/len(ytest)\n",
        "\n",
        "print(\"Baseline Accuracy\")\n",
        "print(f'train {baseline_train_acc:.4f}')\n",
        "print(f'val {baseline_val_acc:.4f}')\n",
        "print(f'test {baseline_test_acc:.4f}')"
      ],
      "metadata": {
        "id": "dTzppfP_b842",
        "outputId": "d71c2a07-9029-4dd9-a7f6-cfedaea9cd52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline Accuracy\n",
            "train 0.5796\n",
            "val 0.5652\n",
            "test 0.5556\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LSTM & GRU\n",
        "single layer"
      ],
      "metadata": {
        "id": "CVRVAdUZd7C1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rnn_model_creator(hunits, rnn_layer, optimizer):\n",
        "    inputs = keras.Input((step, n_feature))\n",
        "    output_layer = keras.layers.Dense(2, activation='softmax')\n",
        "\n",
        "    out = inputs\n",
        "    for hunit in hunits[:-1]:\n",
        "        out = rnn_layer(hunit, return_sequences=True)(out)\n",
        "\n",
        "    out = rnn_layer(hunits[-1])(out)\n",
        "    preds = output_layer(out)\n",
        "\n",
        "    model = keras.Model(inputs, preds)\n",
        "    model.compile(optimizer=optimizer,\n",
        "                    loss='sparse_categorical_crossentropy',\n",
        "                    metrics=['acc'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "2cIp0AE-UlSB"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
        "\n",
        "rnn_layer_repr = ['LSTM', 'GRU']\n",
        "optimizer_repr = ['Adam', 'SGD']\n",
        "for i ,rnn_layer in enumerate([keras.layers.LSTM, keras.layers.GRU]):\n",
        "    for j, optimizer in enumerate([keras.optimizers.Adam, keras.optimizers.SGD]):\n",
        "        for lr in [1e-2, 1e-3, 1e-4]:\n",
        "            for hunit in [8, 32, 64, 256, 512]:\n",
        "                tries = 5\n",
        "                acc_train, acc_val, acc_test = 0,0,0\n",
        "                for _ in range(tries):\n",
        "                    model = rnn_model_creator(hunits=[hunit,], rnn_layer=keras.layers.LSTM, optimizer=optimizer(lr))\n",
        "                    model.fit(xtrain, ytrain, validation_data=(xval, yval), epochs=300, batch_size=8,\n",
        "                            callbacks=[es_callback], verbose=0)\n",
        "\n",
        "                    result_train = model.evaluate(xtrain, ytrain, verbose=0)\n",
        "                    result_val  = model.evaluate(xval, yval, verbose=0)\n",
        "                    result_test = model.evaluate(xtest, ytest, verbose=0)\n",
        "\n",
        "                    acc_train += result_train[1] /tries\n",
        "                    acc_val += result_val[1] /tries\n",
        "                    acc_test += result_test[1] /tries\n",
        "\n",
        "\n",
        "                print('rnn_layer', rnn_layer_repr[i], end=' ')\n",
        "                print('optimizer', optimizer_repr[j], end=' ')\n",
        "                print('hunit', hunit, end=' ')\n",
        "                print('lr', lr, end=' ')\n",
        "                print(f'train acc {acc_train:.2f}', end=' ')\n",
        "                print(f'val acc {acc_val:.2f}', end=' ')\n",
        "                print(f'test acc {acc_test:.2f}', end=' ')\n",
        "                print(f'baseline val {acc_val - baseline_val_acc:.2f}', end=' ')\n",
        "                print(f'baseline test {acc_test - baseline_test_acc:.2f}', end=' ')\n",
        "                print()"
      ],
      "metadata": {
        "id": "FiSFwzo-UMAf",
        "outputId": "4564353d-10e0-4bf3-d269-a26823ed8b22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rnn_layer LSTM optimizer Adam hunit 8 lr 0.01 train acc 0.60 val acc 0.57 test acc 0.54 baseline val -0.00 baseline test -0.01 \n",
            "rnn_layer LSTM optimizer Adam hunit 32 lr 0.01 train acc 0.63 val acc 0.55 test acc 0.54 baseline val -0.02 baseline test -0.02 \n",
            "rnn_layer LSTM optimizer Adam hunit 64 lr 0.01 train acc 0.59 val acc 0.57 test acc 0.54 baseline val -0.00 baseline test -0.01 \n",
            "rnn_layer LSTM optimizer Adam hunit 256 lr 0.01 train acc 0.60 val acc 0.56 test acc 0.54 baseline val -0.01 baseline test -0.01 \n",
            "rnn_layer LSTM optimizer Adam hunit 512 lr 0.01 train acc 0.66 val acc 0.60 test acc 0.58 baseline val 0.03 baseline test 0.03 \n",
            "rnn_layer LSTM optimizer Adam hunit 8 lr 0.001 train acc 0.60 val acc 0.57 test acc 0.56 baseline val -0.00 baseline test 0.00 \n",
            "rnn_layer LSTM optimizer Adam hunit 32 lr 0.001 train acc 0.61 val acc 0.59 test acc 0.56 baseline val 0.03 baseline test 0.00 \n",
            "rnn_layer LSTM optimizer Adam hunit 64 lr 0.001 train acc 0.62 val acc 0.57 test acc 0.58 baseline val 0.01 baseline test 0.02 \n",
            "rnn_layer LSTM optimizer Adam hunit 256 lr 0.001 train acc 0.62 val acc 0.57 test acc 0.57 baseline val -0.00 baseline test 0.01 \n",
            "rnn_layer LSTM optimizer Adam hunit 512 lr 0.001 train acc 0.59 val acc 0.57 test acc 0.55 baseline val -0.00 baseline test -0.00 \n",
            "rnn_layer LSTM optimizer Adam hunit 8 lr 0.0001 train acc 0.65 val acc 0.55 test acc 0.57 baseline val -0.02 baseline test 0.02 \n",
            "rnn_layer LSTM optimizer Adam hunit 32 lr 0.0001 train acc 0.61 val acc 0.58 test acc 0.57 baseline val 0.02 baseline test 0.01 \n",
            "rnn_layer LSTM optimizer Adam hunit 64 lr 0.0001 train acc 0.59 val acc 0.57 test acc 0.56 baseline val 0.01 baseline test 0.00 \n",
            "rnn_layer LSTM optimizer Adam hunit 256 lr 0.0001 train acc 0.60 val acc 0.58 test acc 0.57 baseline val 0.02 baseline test 0.01 \n",
            "rnn_layer LSTM optimizer Adam hunit 512 lr 0.0001 train acc 0.60 val acc 0.57 test acc 0.56 baseline val -0.00 baseline test 0.00 \n",
            "rnn_layer LSTM optimizer SGD hunit 8 lr 0.01 train acc 0.61 val acc 0.59 test acc 0.58 baseline val 0.03 baseline test 0.02 \n",
            "rnn_layer LSTM optimizer SGD hunit 32 lr 0.01 train acc 0.60 val acc 0.57 test acc 0.56 baseline val 0.01 baseline test 0.00 \n",
            "rnn_layer LSTM optimizer SGD hunit 64 lr 0.01 train acc 0.59 val acc 0.57 test acc 0.55 baseline val -0.00 baseline test -0.00 \n",
            "rnn_layer LSTM optimizer SGD hunit 256 lr 0.01 train acc 0.59 val acc 0.57 test acc 0.56 baseline val 0.01 baseline test 0.01 \n",
            "rnn_layer LSTM optimizer SGD hunit 512 lr 0.01 train acc 0.59 val acc 0.57 test acc 0.56 baseline val 0.01 baseline test 0.01 \n",
            "rnn_layer LSTM optimizer SGD hunit 8 lr 0.001 train acc 0.61 val acc 0.57 test acc 0.57 baseline val 0.01 baseline test 0.01 \n",
            "rnn_layer LSTM optimizer SGD hunit 32 lr 0.001 train acc 0.62 val acc 0.57 test acc 0.58 baseline val 0.01 baseline test 0.02 \n",
            "rnn_layer LSTM optimizer SGD hunit 64 lr 0.001 train acc 0.60 val acc 0.58 test acc 0.57 baseline val 0.02 baseline test 0.02 \n",
            "rnn_layer LSTM optimizer SGD hunit 256 lr 0.001 train acc 0.59 val acc 0.57 test acc 0.56 baseline val 0.01 baseline test 0.01 \n",
            "rnn_layer LSTM optimizer SGD hunit 512 lr 0.001 train acc 0.58 val acc 0.57 test acc 0.56 baseline val 0.01 baseline test 0.00 \n",
            "rnn_layer LSTM optimizer SGD hunit 8 lr 0.0001 train acc 0.58 val acc 0.54 test acc 0.55 baseline val -0.03 baseline test -0.01 \n",
            "rnn_layer LSTM optimizer SGD hunit 32 lr 0.0001 train acc 0.58 val acc 0.56 test acc 0.57 baseline val -0.01 baseline test 0.01 \n",
            "rnn_layer LSTM optimizer SGD hunit 64 lr 0.0001 train acc 0.58 val acc 0.57 test acc 0.55 baseline val -0.00 baseline test -0.00 \n",
            "rnn_layer LSTM optimizer SGD hunit 256 lr 0.0001 train acc 0.58 val acc 0.57 test acc 0.56 baseline val -0.00 baseline test 0.00 \n",
            "rnn_layer LSTM optimizer SGD hunit 512 lr 0.0001 train acc 0.58 val acc 0.57 test acc 0.56 baseline val -0.00 baseline test 0.00 \n",
            "rnn_layer GRU optimizer Adam hunit 8 lr 0.01 train acc 0.62 val acc 0.57 test acc 0.58 baseline val 0.01 baseline test 0.02 \n",
            "rnn_layer GRU optimizer Adam hunit 32 lr 0.01 train acc 0.60 val acc 0.57 test acc 0.54 baseline val 0.01 baseline test -0.01 \n",
            "rnn_layer GRU optimizer Adam hunit 64 lr 0.01 train acc 0.64 val acc 0.57 test acc 0.54 baseline val 0.01 baseline test -0.02 \n",
            "rnn_layer GRU optimizer Adam hunit 256 lr 0.01 train acc 0.63 val acc 0.57 test acc 0.56 baseline val -0.00 baseline test 0.00 \n",
            "rnn_layer GRU optimizer Adam hunit 512 lr 0.01 train acc 0.64 val acc 0.56 test acc 0.52 baseline val -0.01 baseline test -0.04 \n",
            "rnn_layer GRU optimizer Adam hunit 8 lr 0.001 train acc 0.62 val acc 0.60 test acc 0.53 baseline val 0.03 baseline test -0.02 \n",
            "rnn_layer GRU optimizer Adam hunit 32 lr 0.001 train acc 0.63 val acc 0.58 test acc 0.56 baseline val 0.02 baseline test 0.01 \n",
            "rnn_layer GRU optimizer Adam hunit 64 lr 0.001 train acc 0.61 val acc 0.58 test acc 0.57 baseline val 0.02 baseline test 0.02 \n",
            "rnn_layer GRU optimizer Adam hunit 256 lr 0.001 train acc 0.65 val acc 0.57 test acc 0.58 baseline val 0.01 baseline test 0.02 \n",
            "rnn_layer GRU optimizer Adam hunit 512 lr 0.001 train acc 0.65 val acc 0.56 test acc 0.56 baseline val -0.01 baseline test 0.00 \n",
            "rnn_layer GRU optimizer Adam hunit 8 lr 0.0001 train acc 0.62 val acc 0.62 test acc 0.55 baseline val 0.05 baseline test -0.01 \n",
            "rnn_layer GRU optimizer Adam hunit 32 lr 0.0001 train acc 0.60 val acc 0.57 test acc 0.56 baseline val -0.00 baseline test 0.01 \n",
            "rnn_layer GRU optimizer Adam hunit 64 lr 0.0001 train acc 0.60 val acc 0.57 test acc 0.56 baseline val -0.00 baseline test 0.01 \n",
            "rnn_layer GRU optimizer Adam hunit 256 lr 0.0001 train acc 0.59 val acc 0.57 test acc 0.55 baseline val 0.01 baseline test -0.00 \n",
            "rnn_layer GRU optimizer Adam hunit 512 lr 0.0001 train acc 0.59 val acc 0.57 test acc 0.56 baseline val 0.01 baseline test 0.01 \n",
            "rnn_layer GRU optimizer SGD hunit 8 lr 0.01 train acc 0.61 val acc 0.59 test acc 0.57 baseline val 0.03 baseline test 0.02 \n",
            "rnn_layer GRU optimizer SGD hunit 32 lr 0.01 train acc 0.60 val acc 0.56 test acc 0.56 baseline val -0.01 baseline test 0.00 \n",
            "rnn_layer GRU optimizer SGD hunit 64 lr 0.01 train acc 0.60 val acc 0.57 test acc 0.56 baseline val 0.01 baseline test 0.00 \n",
            "rnn_layer GRU optimizer SGD hunit 256 lr 0.01 train acc 0.59 val acc 0.57 test acc 0.56 baseline val -0.00 baseline test 0.01 \n",
            "rnn_layer GRU optimizer SGD hunit 512 lr 0.01 train acc 0.59 val acc 0.57 test acc 0.56 baseline val 0.01 baseline test 0.00 \n",
            "rnn_layer GRU optimizer SGD hunit 8 lr 0.001 train acc 0.59 val acc 0.56 test acc 0.56 baseline val -0.01 baseline test 0.00 \n",
            "rnn_layer GRU optimizer SGD hunit 32 lr 0.001 train acc 0.59 val acc 0.56 test acc 0.57 baseline val -0.01 baseline test 0.01 \n",
            "rnn_layer GRU optimizer SGD hunit 64 lr 0.001 train acc 0.58 val acc 0.55 test acc 0.55 baseline val -0.02 baseline test -0.01 \n",
            "rnn_layer GRU optimizer SGD hunit 256 lr 0.001 train acc 0.58 val acc 0.57 test acc 0.55 baseline val 0.01 baseline test -0.01 \n",
            "rnn_layer GRU optimizer SGD hunit 512 lr 0.001 train acc 0.58 val acc 0.57 test acc 0.55 baseline val -0.00 baseline test -0.00 \n",
            "rnn_layer GRU optimizer SGD hunit 8 lr 0.0001 train acc 0.57 val acc 0.57 test acc 0.55 baseline val 0.01 baseline test -0.00 \n",
            "rnn_layer GRU optimizer SGD hunit 32 lr 0.0001 train acc 0.59 val acc 0.56 test acc 0.56 baseline val -0.01 baseline test 0.00 \n",
            "rnn_layer GRU optimizer SGD hunit 64 lr 0.0001 train acc 0.58 val acc 0.57 test acc 0.55 baseline val -0.00 baseline test -0.01 \n",
            "rnn_layer GRU optimizer SGD hunit 256 lr 0.0001 train acc 0.58 val acc 0.57 test acc 0.56 baseline val -0.00 baseline test 0.00 \n",
            "rnn_layer GRU optimizer SGD hunit 512 lr 0.0001 train acc 0.58 val acc 0.57 test acc 0.56 baseline val -0.00 baseline test 0.00 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 3"
      ],
      "metadata": {
        "id": "69MvVnxOg67e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stacked LSTM & GRU"
      ],
      "metadata": {
        "id": "3gOUseJWjfOi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
        "\n",
        "rnn_layer_repr = ['LSTM', 'GRU']\n",
        "optimizer_repr = ['Adam', 'SGD']\n",
        "for i ,rnn_layer in enumerate([keras.layers.LSTM]):\n",
        "    for j, optimizer in enumerate([keras.optimizers.Adam, keras.optimizers.SGD]):\n",
        "        for lr in [1e-2, 1e-3, 1e-4]:\n",
        "            for repeatition in [2, 3, 4]:\n",
        "                for hunit in [8, 32, 64, 256, 512]:\n",
        "                    tries = 5\n",
        "                    acc_train, acc_val, acc_test = 0,0,0\n",
        "                    for _ in range(tries):\n",
        "                        model = rnn_model_creator(hunits=[hunit]*repeatition, rnn_layer=keras.layers.LSTM, optimizer=optimizer(lr))\n",
        "                        model.fit(xtrain, ytrain, validation_data=(xval, yval), epochs=300, batch_size=8,\n",
        "                                callbacks=[es_callback], verbose=0)\n",
        "\n",
        "                        result_train = model.evaluate(xtrain, ytrain, verbose=0)\n",
        "                        result_val  = model.evaluate(xval, yval, verbose=0)\n",
        "                        result_test = model.evaluate(xtest, ytest, verbose=0)\n",
        "\n",
        "                        acc_train += result_train[1] /tries\n",
        "                        acc_val += result_val[1] /tries\n",
        "                        acc_test += result_test[1] /tries\n",
        "\n",
        "\n",
        "                    print('rnn_layer', rnn_layer_repr[i], end=' ')\n",
        "                    print('optimizer', optimizer_repr[j], end=' ')\n",
        "                    print('hunit', hunit, end=' ')\n",
        "                    print('repeatition', repeatition, end=' ')\n",
        "                    print('lr', lr, end=' ')\n",
        "                    print(f'train acc {acc_train:.2f}', end=' ')\n",
        "                    print(f'val acc {acc_val:.2f}', end=' ')\n",
        "                    print(f'test acc {acc_test:.2f}', end=' ')\n",
        "                    print(f'baseline val {acc_val - baseline_val_acc:.2f}', end=' ')\n",
        "                    print(f'baseline test {acc_test - baseline_test_acc:.2f}', end=' ')\n",
        "                    print()\n",
        "                print('------------ repeatition ---------------')\n",
        "            print('------------ learning rate ---------------')\n",
        "        print('------------ Optimizer ---------------')\n",
        "    print('------------ Cell ---------------')"
      ],
      "metadata": {
        "id": "OecRuJbjZTlz",
        "outputId": "bb738b8b-a4ac-41fc-f877-4669568d3969",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rnn_layer LSTM optimizer Adam hunit 8 repeatition 2 lr 0.01 train acc 0.66 val acc 0.57 test acc 0.55 baseline val -0.00 baseline test -0.00 \n",
            "rnn_layer LSTM optimizer Adam hunit 32 repeatition 2 lr 0.01 train acc 0.65 val acc 0.59 test acc 0.53 baseline val 0.03 baseline test -0.02 \n",
            "rnn_layer LSTM optimizer Adam hunit 64 repeatition 2 lr 0.01 train acc 0.66 val acc 0.63 test acc 0.56 baseline val 0.06 baseline test 0.01 \n",
            "rnn_layer LSTM optimizer Adam hunit 256 repeatition 2 lr 0.01 train acc 0.67 val acc 0.63 test acc 0.58 baseline val 0.07 baseline test 0.03 \n",
            "rnn_layer LSTM optimizer Adam hunit 512 repeatition 2 lr 0.01 train acc 0.58 val acc 0.57 test acc 0.56 baseline val -0.00 baseline test 0.00 \n",
            "------------ repeatition ---------------\n",
            "rnn_layer LSTM optimizer Adam hunit 8 repeatition 3 lr 0.01 train acc 0.64 val acc 0.64 test acc 0.58 baseline val 0.08 baseline test 0.02 \n",
            "rnn_layer LSTM optimizer Adam hunit 32 repeatition 3 lr 0.01 train acc 0.64 val acc 0.58 test acc 0.57 baseline val 0.02 baseline test 0.02 \n",
            "rnn_layer LSTM optimizer Adam hunit 64 repeatition 3 lr 0.01 train acc 0.65 val acc 0.62 test acc 0.60 baseline val 0.05 baseline test 0.04 \n",
            "rnn_layer LSTM optimizer Adam hunit 256 repeatition 3 lr 0.01 train acc 0.58 val acc 0.57 test acc 0.56 baseline val -0.00 baseline test 0.00 \n",
            "rnn_layer LSTM optimizer Adam hunit 512 repeatition 3 lr 0.01 train acc 0.58 val acc 0.57 test acc 0.56 baseline val -0.00 baseline test 0.00 \n",
            "------------ repeatition ---------------\n",
            "rnn_layer LSTM optimizer Adam hunit 8 repeatition 4 lr 0.01 train acc 0.62 val acc 0.64 test acc 0.58 baseline val 0.08 baseline test 0.03 \n",
            "rnn_layer LSTM optimizer Adam hunit 32 repeatition 4 lr 0.01 train acc 0.59 val acc 0.57 test acc 0.57 baseline val 0.01 baseline test 0.02 \n",
            "rnn_layer LSTM optimizer Adam hunit 64 repeatition 4 lr 0.01 train acc 0.58 val acc 0.57 test acc 0.56 baseline val -0.00 baseline test 0.00 \n",
            "rnn_layer LSTM optimizer Adam hunit 256 repeatition 4 lr 0.01 train acc 0.58 val acc 0.57 test acc 0.56 baseline val -0.00 baseline test 0.00 \n",
            "rnn_layer LSTM optimizer Adam hunit 512 repeatition 4 lr 0.01 train acc 0.58 val acc 0.57 test acc 0.56 baseline val -0.00 baseline test 0.00 \n",
            "------------ repeatition ---------------\n",
            "------------ learning rate ---------------\n",
            "rnn_layer LSTM optimizer Adam hunit 8 repeatition 2 lr 0.001 train acc 0.63 val acc 0.64 test acc 0.61 baseline val 0.08 baseline test 0.05 \n",
            "rnn_layer LSTM optimizer Adam hunit 32 repeatition 2 lr 0.001 train acc 0.64 val acc 0.57 test acc 0.61 baseline val 0.01 baseline test 0.05 \n",
            "rnn_layer LSTM optimizer Adam hunit 64 repeatition 2 lr 0.001 train acc 0.63 val acc 0.63 test acc 0.60 baseline val 0.06 baseline test 0.04 \n",
            "rnn_layer LSTM optimizer Adam hunit 256 repeatition 2 lr 0.001 train acc 0.62 val acc 0.59 test acc 0.57 baseline val 0.03 baseline test 0.02 \n",
            "rnn_layer LSTM optimizer Adam hunit 512 repeatition 2 lr 0.001 train acc 0.58 val acc 0.57 test acc 0.56 baseline val -0.00 baseline test 0.00 \n",
            "------------ repeatition ---------------\n",
            "rnn_layer LSTM optimizer Adam hunit 8 repeatition 3 lr 0.001 train acc 0.62 val acc 0.66 test acc 0.62 baseline val 0.10 baseline test 0.07 \n",
            "rnn_layer LSTM optimizer Adam hunit 32 repeatition 3 lr 0.001 train acc 0.63 val acc 0.63 test acc 0.62 baseline val 0.07 baseline test 0.06 \n",
            "rnn_layer LSTM optimizer Adam hunit 64 repeatition 3 lr 0.001 train acc 0.64 val acc 0.58 test acc 0.60 baseline val 0.02 baseline test 0.05 \n",
            "rnn_layer LSTM optimizer Adam hunit 256 repeatition 3 lr 0.001 train acc 0.62 val acc 0.58 test acc 0.60 baseline val 0.02 baseline test 0.05 \n",
            "rnn_layer LSTM optimizer Adam hunit 512 repeatition 3 lr 0.001 train acc 0.61 val acc 0.61 test acc 0.58 baseline val 0.04 baseline test 0.03 \n",
            "------------ repeatition ---------------\n",
            "rnn_layer LSTM optimizer Adam hunit 8 repeatition 4 lr 0.001 train acc 0.62 val acc 0.61 test acc 0.58 baseline val 0.04 baseline test 0.03 \n",
            "rnn_layer LSTM optimizer Adam hunit 32 repeatition 4 lr 0.001 train acc 0.64 val acc 0.63 test acc 0.62 baseline val 0.06 baseline test 0.06 \n",
            "rnn_layer LSTM optimizer Adam hunit 64 repeatition 4 lr 0.001 train acc 0.65 val acc 0.64 test acc 0.61 baseline val 0.08 baseline test 0.05 \n",
            "rnn_layer LSTM optimizer Adam hunit 256 repeatition 4 lr 0.001 train acc 0.66 val acc 0.63 test acc 0.60 baseline val 0.06 baseline test 0.04 \n",
            "rnn_layer LSTM optimizer Adam hunit 512 repeatition 4 lr 0.001 train acc 0.60 val acc 0.64 test acc 0.59 baseline val 0.08 baseline test 0.04 \n",
            "------------ repeatition ---------------\n",
            "------------ learning rate ---------------\n",
            "rnn_layer LSTM optimizer Adam hunit 8 repeatition 2 lr 0.0001 train acc 0.63 val acc 0.60 test acc 0.60 baseline val 0.03 baseline test 0.05 \n",
            "rnn_layer LSTM optimizer Adam hunit 32 repeatition 2 lr 0.0001 train acc 0.65 val acc 0.60 test acc 0.60 baseline val 0.03 baseline test 0.04 \n",
            "rnn_layer LSTM optimizer Adam hunit 64 repeatition 2 lr 0.0001 train acc 0.64 val acc 0.57 test acc 0.60 baseline val 0.01 baseline test 0.04 \n",
            "rnn_layer LSTM optimizer Adam hunit 256 repeatition 2 lr 0.0001 train acc 0.61 val acc 0.61 test acc 0.59 baseline val 0.04 baseline test 0.03 \n",
            "rnn_layer LSTM optimizer Adam hunit 512 repeatition 2 lr 0.0001 train acc 0.65 val acc 0.59 test acc 0.58 baseline val 0.03 baseline test 0.02 \n",
            "------------ repeatition ---------------\n",
            "rnn_layer LSTM optimizer Adam hunit 8 repeatition 3 lr 0.0001 train acc 0.63 val acc 0.62 test acc 0.62 baseline val 0.05 baseline test 0.07 \n",
            "rnn_layer LSTM optimizer Adam hunit 32 repeatition 3 lr 0.0001 train acc 0.65 val acc 0.57 test acc 0.61 baseline val 0.01 baseline test 0.06 \n",
            "rnn_layer LSTM optimizer Adam hunit 64 repeatition 3 lr 0.0001 train acc 0.64 val acc 0.57 test acc 0.64 baseline val 0.01 baseline test 0.08 \n",
            "rnn_layer LSTM optimizer Adam hunit 256 repeatition 3 lr 0.0001 train acc 0.64 val acc 0.60 test acc 0.62 baseline val 0.03 baseline test 0.07 \n",
            "rnn_layer LSTM optimizer Adam hunit 512 repeatition 3 lr 0.0001 train acc 0.65 val acc 0.57 test acc 0.62 baseline val -0.00 baseline test 0.07 \n",
            "------------ repeatition ---------------\n",
            "rnn_layer LSTM optimizer Adam hunit 8 repeatition 4 lr 0.0001 train acc 0.64 val acc 0.63 test acc 0.61 baseline val 0.07 baseline test 0.05 \n",
            "rnn_layer LSTM optimizer Adam hunit 32 repeatition 4 lr 0.0001 train acc 0.64 val acc 0.67 test acc 0.61 baseline val 0.10 baseline test 0.06 \n",
            "rnn_layer LSTM optimizer Adam hunit 64 repeatition 4 lr 0.0001 train acc 0.63 val acc 0.61 test acc 0.64 baseline val 0.04 baseline test 0.08 \n",
            "rnn_layer LSTM optimizer Adam hunit 256 repeatition 4 lr 0.0001 train acc 0.64 val acc 0.63 test acc 0.63 baseline val 0.06 baseline test 0.07 \n",
            "rnn_layer LSTM optimizer Adam hunit 512 repeatition 4 lr 0.0001 train acc 0.63 val acc 0.58 test acc 0.60 baseline val 0.02 baseline test 0.04 \n",
            "------------ repeatition ---------------\n",
            "------------ learning rate ---------------\n",
            "------------ Optimizer ---------------\n",
            "rnn_layer LSTM optimizer SGD hunit 8 repeatition 2 lr 0.01 train acc 0.61 val acc 0.61 test acc 0.56 baseline val 0.04 baseline test 0.01 \n",
            "rnn_layer LSTM optimizer SGD hunit 32 repeatition 2 lr 0.01 train acc 0.63 val acc 0.59 test acc 0.61 baseline val 0.03 baseline test 0.06 \n",
            "rnn_layer LSTM optimizer SGD hunit 64 repeatition 2 lr 0.01 train acc 0.59 val acc 0.57 test acc 0.57 baseline val 0.01 baseline test 0.02 \n",
            "rnn_layer LSTM optimizer SGD hunit 256 repeatition 2 lr 0.01 train acc 0.59 val acc 0.57 test acc 0.56 baseline val 0.01 baseline test 0.01 \n",
            "rnn_layer LSTM optimizer SGD hunit 512 repeatition 2 lr 0.01 train acc 0.59 val acc 0.57 test acc 0.57 baseline val -0.00 baseline test 0.01 \n",
            "------------ repeatition ---------------\n",
            "rnn_layer LSTM optimizer SGD hunit 8 repeatition 3 lr 0.01 train acc 0.61 val acc 0.61 test acc 0.58 baseline val 0.04 baseline test 0.03 \n",
            "rnn_layer LSTM optimizer SGD hunit 32 repeatition 3 lr 0.01 train acc 0.63 val acc 0.57 test acc 0.62 baseline val 0.01 baseline test 0.07 \n",
            "rnn_layer LSTM optimizer SGD hunit 64 repeatition 3 lr 0.01 train acc 0.64 val acc 0.60 test acc 0.64 baseline val 0.03 baseline test 0.08 \n",
            "rnn_layer LSTM optimizer SGD hunit 256 repeatition 3 lr 0.01 train acc 0.59 val acc 0.59 test acc 0.57 baseline val 0.03 baseline test 0.02 \n",
            "rnn_layer LSTM optimizer SGD hunit 512 repeatition 3 lr 0.01 train acc 0.63 val acc 0.60 test acc 0.62 baseline val 0.03 baseline test 0.07 \n",
            "------------ repeatition ---------------\n",
            "rnn_layer LSTM optimizer SGD hunit 8 repeatition 4 lr 0.01 train acc 0.62 val acc 0.63 test acc 0.63 baseline val 0.07 baseline test 0.08 \n",
            "rnn_layer LSTM optimizer SGD hunit 32 repeatition 4 lr 0.01 train acc 0.64 val acc 0.61 test acc 0.64 baseline val 0.04 baseline test 0.08 \n",
            "rnn_layer LSTM optimizer SGD hunit 64 repeatition 4 lr 0.01 train acc 0.64 val acc 0.57 test acc 0.63 baseline val 0.01 baseline test 0.08 \n",
            "rnn_layer LSTM optimizer SGD hunit 256 repeatition 4 lr 0.01 train acc 0.65 val acc 0.56 test acc 0.64 baseline val -0.01 baseline test 0.09 \n",
            "rnn_layer LSTM optimizer SGD hunit 512 repeatition 4 lr 0.01 train acc 0.64 val acc 0.59 test acc 0.65 baseline val 0.03 baseline test 0.10 \n",
            "------------ repeatition ---------------\n",
            "------------ learning rate ---------------\n",
            "rnn_layer LSTM optimizer SGD hunit 8 repeatition 2 lr 0.001 train acc 0.60 val acc 0.61 test acc 0.57 baseline val 0.04 baseline test 0.02 \n",
            "rnn_layer LSTM optimizer SGD hunit 32 repeatition 2 lr 0.001 train acc 0.61 val acc 0.57 test acc 0.59 baseline val 0.01 baseline test 0.03 \n",
            "rnn_layer LSTM optimizer SGD hunit 64 repeatition 2 lr 0.001 train acc 0.60 val acc 0.57 test acc 0.60 baseline val 0.01 baseline test 0.04 \n",
            "rnn_layer LSTM optimizer SGD hunit 256 repeatition 2 lr 0.001 train acc 0.59 val acc 0.58 test acc 0.58 baseline val 0.02 baseline test 0.03 \n",
            "rnn_layer LSTM optimizer SGD hunit 512 repeatition 2 lr 0.001 train acc 0.59 val acc 0.57 test acc 0.56 baseline val -0.00 baseline test 0.01 \n",
            "------------ repeatition ---------------\n",
            "rnn_layer LSTM optimizer SGD hunit 8 repeatition 3 lr 0.001 train acc 0.58 val acc 0.57 test acc 0.56 baseline val -0.00 baseline test 0.00 \n",
            "rnn_layer LSTM optimizer SGD hunit 32 repeatition 3 lr 0.001 train acc 0.58 val acc 0.57 test acc 0.56 baseline val -0.00 baseline test 0.00 \n",
            "rnn_layer LSTM optimizer SGD hunit 64 repeatition 3 lr 0.001 train acc 0.58 val acc 0.57 test acc 0.56 baseline val -0.00 baseline test 0.00 \n",
            "rnn_layer LSTM optimizer SGD hunit 256 repeatition 3 lr 0.001 train acc 0.58 val acc 0.57 test acc 0.56 baseline val -0.00 baseline test 0.00 \n",
            "rnn_layer LSTM optimizer SGD hunit 512 repeatition 3 lr 0.001 train acc 0.58 val acc 0.57 test acc 0.56 baseline val -0.00 baseline test 0.00 \n",
            "------------ repeatition ---------------\n",
            "rnn_layer LSTM optimizer SGD hunit 8 repeatition 4 lr 0.001 train acc 0.58 val acc 0.57 test acc 0.56 baseline val -0.00 baseline test 0.00 \n",
            "rnn_layer LSTM optimizer SGD hunit 32 repeatition 4 lr 0.001 train acc 0.58 val acc 0.57 test acc 0.56 baseline val -0.00 baseline test 0.00 \n",
            "rnn_layer LSTM optimizer SGD hunit 64 repeatition 4 lr 0.001 train acc 0.58 val acc 0.57 test acc 0.56 baseline val -0.00 baseline test 0.00 \n",
            "rnn_layer LSTM optimizer SGD hunit 256 repeatition 4 lr 0.001 train acc 0.58 val acc 0.57 test acc 0.56 baseline val -0.00 baseline test 0.00 \n",
            "rnn_layer LSTM optimizer SGD hunit 512 repeatition 4 lr 0.001 train acc 0.58 val acc 0.57 test acc 0.56 baseline val -0.00 baseline test 0.00 \n",
            "------------ repeatition ---------------\n",
            "------------ learning rate ---------------\n",
            "rnn_layer LSTM optimizer SGD hunit 8 repeatition 2 lr 0.0001 train acc 0.58 val acc 0.57 test acc 0.56 baseline val -0.00 baseline test 0.00 \n",
            "rnn_layer LSTM optimizer SGD hunit 32 repeatition 2 lr 0.0001 train acc 0.57 val acc 0.57 test acc 0.55 baseline val -0.00 baseline test -0.01 \n",
            "rnn_layer LSTM optimizer SGD hunit 64 repeatition 2 lr 0.0001 train acc 0.58 val acc 0.57 test acc 0.56 baseline val -0.00 baseline test 0.00 \n",
            "rnn_layer LSTM optimizer SGD hunit 256 repeatition 2 lr 0.0001 train acc 0.58 val acc 0.57 test acc 0.56 baseline val -0.00 baseline test 0.00 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
        "\n",
        "rnn_layer_repr = ['GRU']\n",
        "optimizer_repr = ['Adam', 'SGD']\n",
        "for i ,rnn_layer in enumerate([keras.layers.GRU]):\n",
        "    for j, optimizer in enumerate([keras.optimizers.Adam, keras.optimizers.SGD]):\n",
        "        for lr in [1e-2, 1e-3, 1e-4]:\n",
        "            for repeatition in [2, 3, 4]:\n",
        "                for hunit in [8, 32, 64, 256, 512]:\n",
        "                    tries = 5\n",
        "                    acc_train, acc_val, acc_test = 0,0,0\n",
        "                    for _ in range(tries):\n",
        "                        model = rnn_model_creator(hunits=[hunit]*repeatition, rnn_layer=keras.layers.LSTM, optimizer=optimizer(lr))\n",
        "                        model.fit(xtrain, ytrain, validation_data=(xval, yval), epochs=300, batch_size=8,\n",
        "                                callbacks=[es_callback], verbose=0)\n",
        "\n",
        "                        result_train = model.evaluate(xtrain, ytrain, verbose=0)\n",
        "                        result_val  = model.evaluate(xval, yval, verbose=0)\n",
        "                        result_test = model.evaluate(xtest, ytest, verbose=0)\n",
        "\n",
        "                        acc_train += result_train[1] /tries\n",
        "                        acc_val += result_val[1] /tries\n",
        "                        acc_test += result_test[1] /tries\n",
        "\n",
        "\n",
        "                    print('rnn_layer', rnn_layer_repr[i], end=' ')\n",
        "                    print('optimizer', optimizer_repr[j], end=' ')\n",
        "                    print('hunit', hunit, end=' ')\n",
        "                    print('repeatition', repeatition, end=' ')\n",
        "                    print('lr', lr, end=' ')\n",
        "                    print(f'train acc {acc_train:.2f}', end=' ')\n",
        "                    print(f'val acc {acc_val:.2f}', end=' ')\n",
        "                    print(f'test acc {acc_test:.2f}', end=' ')\n",
        "                    print(f'baseline val {acc_val - baseline_val_acc:.2f}', end=' ')\n",
        "                    print(f'baseline test {acc_test - baseline_test_acc:.2f}', end=' ')\n",
        "                    print()\n",
        "                print('------------ repeatition ---------------')\n",
        "            print('------------ learning rate ---------------')\n",
        "        print('------------ Optimizer ---------------')\n",
        "    print('------------ Cell ---------------')"
      ],
      "metadata": {
        "id": "AtQuRgC6Xj4D",
        "outputId": "6b02a4b0-efcc-4c92-b64a-78a4cf7f7774",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rnn_layer GRU optimizer Adam hunit 8 repeatition 2 lr 0.01 train acc 0.63 val acc 0.57 test acc 0.56 baseline val 0.01 baseline test 0.00 \n",
            "rnn_layer GRU optimizer Adam hunit 32 repeatition 2 lr 0.01 train acc 0.65 val acc 0.57 test acc 0.54 baseline val 0.01 baseline test -0.01 \n",
            "rnn_layer GRU optimizer Adam hunit 64 repeatition 2 lr 0.01 train acc 0.64 val acc 0.57 test acc 0.55 baseline val -0.00 baseline test -0.00 \n",
            "rnn_layer GRU optimizer Adam hunit 256 repeatition 2 lr 0.01 train acc 0.60 val acc 0.60 test acc 0.57 baseline val 0.03 baseline test 0.02 \n",
            "rnn_layer GRU optimizer Adam hunit 512 repeatition 2 lr 0.01 train acc 0.58 val acc 0.57 test acc 0.56 baseline val -0.00 baseline test 0.00 \n",
            "------------ repeatition ---------------\n",
            "rnn_layer GRU optimizer Adam hunit 8 repeatition 3 lr 0.01 train acc 0.65 val acc 0.62 test acc 0.60 baseline val 0.05 baseline test 0.04 \n",
            "rnn_layer GRU optimizer Adam hunit 32 repeatition 3 lr 0.01 train acc 0.62 val acc 0.60 test acc 0.58 baseline val 0.03 baseline test 0.03 \n",
            "rnn_layer GRU optimizer Adam hunit 64 repeatition 3 lr 0.01 train acc 0.65 val acc 0.61 test acc 0.56 baseline val 0.04 baseline test 0.00 \n",
            "rnn_layer GRU optimizer Adam hunit 256 repeatition 3 lr 0.01 train acc 0.58 val acc 0.57 test acc 0.56 baseline val -0.00 baseline test 0.00 \n",
            "rnn_layer GRU optimizer Adam hunit 512 repeatition 3 lr 0.01 train acc 0.58 val acc 0.57 test acc 0.56 baseline val -0.00 baseline test 0.00 \n",
            "------------ repeatition ---------------\n",
            "rnn_layer GRU optimizer Adam hunit 8 repeatition 4 lr 0.01 train acc 0.59 val acc 0.61 test acc 0.59 baseline val 0.04 baseline test 0.03 \n",
            "rnn_layer GRU optimizer Adam hunit 32 repeatition 4 lr 0.01 train acc 0.58 val acc 0.57 test acc 0.56 baseline val -0.00 baseline test 0.00 \n",
            "rnn_layer GRU optimizer Adam hunit 64 repeatition 4 lr 0.01 train acc 0.58 val acc 0.57 test acc 0.56 baseline val -0.00 baseline test 0.00 \n",
            "rnn_layer GRU optimizer Adam hunit 256 repeatition 4 lr 0.01 train acc 0.58 val acc 0.57 test acc 0.56 baseline val -0.00 baseline test 0.00 \n",
            "rnn_layer GRU optimizer Adam hunit 512 repeatition 4 lr 0.01 train acc 0.58 val acc 0.57 test acc 0.56 baseline val -0.00 baseline test 0.00 \n",
            "------------ repeatition ---------------\n",
            "------------ learning rate ---------------\n",
            "rnn_layer GRU optimizer Adam hunit 8 repeatition 2 lr 0.001 train acc 0.64 val acc 0.60 test acc 0.60 baseline val 0.03 baseline test 0.04 \n",
            "rnn_layer GRU optimizer Adam hunit 32 repeatition 2 lr 0.001 train acc 0.62 val acc 0.63 test acc 0.60 baseline val 0.07 baseline test 0.05 \n",
            "rnn_layer GRU optimizer Adam hunit 64 repeatition 2 lr 0.001 train acc 0.62 val acc 0.57 test acc 0.59 baseline val -0.00 baseline test 0.03 \n",
            "rnn_layer GRU optimizer Adam hunit 256 repeatition 2 lr 0.001 train acc 0.62 val acc 0.59 test acc 0.57 baseline val 0.03 baseline test 0.02 \n",
            "rnn_layer GRU optimizer Adam hunit 512 repeatition 2 lr 0.001 train acc 0.63 val acc 0.60 test acc 0.58 baseline val 0.03 baseline test 0.02 \n",
            "------------ repeatition ---------------\n",
            "rnn_layer GRU optimizer Adam hunit 8 repeatition 3 lr 0.001 train acc 0.63 val acc 0.63 test acc 0.61 baseline val 0.06 baseline test 0.06 \n",
            "rnn_layer GRU optimizer Adam hunit 32 repeatition 3 lr 0.001 train acc 0.63 val acc 0.63 test acc 0.59 baseline val 0.06 baseline test 0.04 \n",
            "rnn_layer GRU optimizer Adam hunit 64 repeatition 3 lr 0.001 train acc 0.62 val acc 0.61 test acc 0.60 baseline val 0.04 baseline test 0.05 \n",
            "rnn_layer GRU optimizer Adam hunit 256 repeatition 3 lr 0.001 train acc 0.63 val acc 0.59 test acc 0.59 baseline val 0.03 baseline test 0.04 \n",
            "rnn_layer GRU optimizer Adam hunit 512 repeatition 3 lr 0.001 train acc 0.64 val acc 0.59 test acc 0.58 baseline val 0.03 baseline test 0.03 \n",
            "------------ repeatition ---------------\n",
            "rnn_layer GRU optimizer Adam hunit 8 repeatition 4 lr 0.001 train acc 0.62 val acc 0.63 test acc 0.60 baseline val 0.06 baseline test 0.04 \n",
            "rnn_layer GRU optimizer Adam hunit 32 repeatition 4 lr 0.001 train acc 0.63 val acc 0.61 test acc 0.59 baseline val 0.04 baseline test 0.03 \n",
            "rnn_layer GRU optimizer Adam hunit 64 repeatition 4 lr 0.001 train acc 0.61 val acc 0.63 test acc 0.62 baseline val 0.07 baseline test 0.06 \n",
            "rnn_layer GRU optimizer Adam hunit 256 repeatition 4 lr 0.001 train acc 0.63 val acc 0.61 test acc 0.62 baseline val 0.04 baseline test 0.06 \n",
            "rnn_layer GRU optimizer Adam hunit 512 repeatition 4 lr 0.001 train acc 0.59 val acc 0.58 test acc 0.57 baseline val 0.02 baseline test 0.02 \n",
            "------------ repeatition ---------------\n",
            "------------ learning rate ---------------\n",
            "rnn_layer GRU optimizer Adam hunit 8 repeatition 2 lr 0.0001 train acc 0.66 val acc 0.59 test acc 0.60 baseline val 0.03 baseline test 0.04 \n",
            "rnn_layer GRU optimizer Adam hunit 32 repeatition 2 lr 0.0001 train acc 0.64 val acc 0.62 test acc 0.61 baseline val 0.05 baseline test 0.05 \n",
            "rnn_layer GRU optimizer Adam hunit 64 repeatition 2 lr 0.0001 train acc 0.64 val acc 0.55 test acc 0.60 baseline val -0.02 baseline test 0.04 \n",
            "rnn_layer GRU optimizer Adam hunit 256 repeatition 2 lr 0.0001 train acc 0.63 val acc 0.59 test acc 0.59 baseline val 0.03 baseline test 0.04 \n",
            "rnn_layer GRU optimizer Adam hunit 512 repeatition 2 lr 0.0001 train acc 0.64 val acc 0.57 test acc 0.58 baseline val 0.01 baseline test 0.03 \n",
            "------------ repeatition ---------------\n",
            "rnn_layer GRU optimizer Adam hunit 8 repeatition 3 lr 0.0001 train acc 0.62 val acc 0.65 test acc 0.60 baseline val 0.09 baseline test 0.04 \n",
            "rnn_layer GRU optimizer Adam hunit 32 repeatition 3 lr 0.0001 train acc 0.64 val acc 0.63 test acc 0.61 baseline val 0.06 baseline test 0.06 \n",
            "rnn_layer GRU optimizer Adam hunit 64 repeatition 3 lr 0.0001 train acc 0.65 val acc 0.60 test acc 0.62 baseline val 0.03 baseline test 0.07 \n",
            "rnn_layer GRU optimizer Adam hunit 256 repeatition 3 lr 0.0001 train acc 0.65 val acc 0.57 test acc 0.64 baseline val -0.00 baseline test 0.08 \n",
            "rnn_layer GRU optimizer Adam hunit 512 repeatition 3 lr 0.0001 train acc 0.65 val acc 0.55 test acc 0.59 baseline val -0.02 baseline test 0.03 \n",
            "------------ repeatition ---------------\n",
            "rnn_layer GRU optimizer Adam hunit 8 repeatition 4 lr 0.0001 train acc 0.61 val acc 0.61 test acc 0.59 baseline val 0.04 baseline test 0.04 \n",
            "rnn_layer GRU optimizer Adam hunit 32 repeatition 4 lr 0.0001 train acc 0.63 val acc 0.63 test acc 0.63 baseline val 0.06 baseline test 0.07 \n",
            "rnn_layer GRU optimizer Adam hunit 64 repeatition 4 lr 0.0001 train acc 0.63 val acc 0.64 test acc 0.63 baseline val 0.08 baseline test 0.08 \n",
            "rnn_layer GRU optimizer Adam hunit 256 repeatition 4 lr 0.0001 train acc 0.62 val acc 0.63 test acc 0.60 baseline val 0.06 baseline test 0.05 \n",
            "rnn_layer GRU optimizer Adam hunit 512 repeatition 4 lr 0.0001 train acc 0.64 val acc 0.60 test acc 0.64 baseline val 0.03 baseline test 0.08 \n",
            "------------ repeatition ---------------\n",
            "------------ learning rate ---------------\n",
            "------------ Optimizer ---------------\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "code.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}