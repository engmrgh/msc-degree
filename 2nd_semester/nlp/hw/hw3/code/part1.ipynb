{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpW6jJo2w2iM"
      },
      "source": [
        "<div align=\"center\"><h1>HW3, Part1</h1></div>\n",
        "<div align=\"center\"><h2>Mohammadreza Ghofrani, 400131076</h2></div>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --no-deps bert-embedding mxnet gluonnlp mxnet-cu92"
      ],
      "metadata": {
        "id": "t3bqk5_Ws1xl",
        "outputId": "07b663db-2265-470a-cfa6-c3d04187cba6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting bert-embedding\n",
            "  Downloading bert_embedding-1.0.1-py3-none-any.whl (13 kB)\n",
            "Collecting mxnet\n",
            "  Downloading mxnet-1.9.1-py3-none-manylinux2014_x86_64.whl (49.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 49.1 MB 1.9 MB/s \n",
            "\u001b[?25hCollecting gluonnlp\n",
            "  Downloading gluonnlp-0.10.0.tar.gz (344 kB)\n",
            "\u001b[K     |████████████████████████████████| 344 kB 55.7 MB/s \n",
            "\u001b[?25hCollecting mxnet-cu92\n",
            "  Downloading mxnet_cu92-1.7.0-py2.py3-none-manylinux2014_x86_64.whl (789.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 789.8 MB 15 kB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: gluonnlp\n",
            "  Building wheel for gluonnlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gluonnlp: filename=gluonnlp-0.10.0-cp37-cp37m-linux_x86_64.whl size=595732 sha256=6c258d15c02c23fb8683c0c69faa4aca662c36395a039aa554337f51e3d3e9b6\n",
            "  Stored in directory: /root/.cache/pip/wheels/be/b4/06/7f3fdfaf707e6b5e98b79c041e023acffbe395d78a527eae00\n",
            "Successfully built gluonnlp\n",
            "Installing collected packages: mxnet-cu92, mxnet, gluonnlp, bert-embedding\n",
            "Successfully installed bert-embedding-1.0.1 gluonnlp-0.10.0 mxnet-1.9.1 mxnet-cu92-1.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Qw6dCtEPw2iS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import mxnet as mx\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sklearn import svm\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from bert_embedding import BertEmbedding"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "If39KgbI_pFS",
        "outputId": "d5dbd276-d2bd-45e3-cc9a-d2fc23d0df1c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "Dmdb8bh-w4sr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1oin_Sw1Gk_WLS9zpDrap5FdlRcUrTP_D\n",
        "!gdown 1EvtGQ8-sYXXQ3VA9ByjD4OmFH13WxvAs\n",
        "!gdown 154f-z0PsPAp0yvOLdNXP8NXYjPljpgCZ\n",
        "!gdown 1EDG_j6F5ohIjpkihRQd0-i9daHWV7StN\n",
        "!mkdir data\n",
        "!mv *.txt data/"
      ],
      "metadata": {
        "id": "rg1ogygrw3L3",
        "outputId": "5122b203-e28d-4856-f1e1-cfffd485e2a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1oin_Sw1Gk_WLS9zpDrap5FdlRcUrTP_D\n",
            "To: /content/Sentenses_train.txt\n",
            "100% 1.23M/1.23M [00:00<00:00, 73.8MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1EvtGQ8-sYXXQ3VA9ByjD4OmFH13WxvAs\n",
            "To: /content/Senses_train.txt\n",
            "100% 49.7k/49.7k [00:00<00:00, 51.1MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=154f-z0PsPAp0yvOLdNXP8NXYjPljpgCZ\n",
            "To: /content/Sentenses_test.txt\n",
            "100% 213k/213k [00:00<00:00, 14.6MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1EDG_j6F5ohIjpkihRQd0-i9daHWV7StN\n",
            "To: /content/Senses_test.txt\n",
            "100% 8.76k/8.76k [00:00<00:00, 7.97MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read files and embeddings"
      ],
      "metadata": {
        "id": "sTugpexIM4d5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ds_reader(ds_name, context_radius=10):\n",
        "    dataset = {'word':[], 'embedding':[], 'sense':[]}\n",
        "    bert_embedding = BertEmbedding()\n",
        "    with open(f'data/Sentenses_{ds_name}.txt', 'r', encoding='utf-8') as fsentence, \\\n",
        "         open(f'data/Senses_{ds_name}.txt', 'r', encoding='utf-8') as fsense:\n",
        "        senses = fsense.read().split('\\n')\n",
        "        sentences = fsentence.read().split('\\n')\n",
        "        for sentence, sense in tqdm(zip(sentences, senses), total=len(sentences)):\n",
        "            match = re.search('<head>(.*?)</head>', sentence)\n",
        "            if match:\n",
        "                target = match.group(0) \\\n",
        "                    .replace('<head>', '') \\\n",
        "                    .replace('</head>', '') \\\n",
        "                    .strip() \\\n",
        "                    .lower()\n",
        "                mytokens = sentence.replace('<head>', '') \\\n",
        "                                .replace('</head>', '') \\\n",
        "                                .lower() \\\n",
        "                                .split(' ')\n",
        "                context = ' '.join(mytokens[max(0, mytokens.index(target) - context_radius): \\\n",
        "                                            min(mytokens.index(target) + context_radius, len(mytokens))])\n",
        "                berttokens, embeds = bert_embedding([context,])[0]\n",
        "                target_index = berttokens.index(target)\n",
        "                dataset['word'].append(target)\n",
        "                dataset['embedding'].append(embeds[target_index])\n",
        "                dataset['sense'].append(sense)\n",
        "    return pd.DataFrame(dataset)"
      ],
      "metadata": {
        "id": "3zHRkK5vUubt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataframe = ds_reader('train')\n",
        "test_dataframe = ds_reader('test')"
      ],
      "metadata": {
        "id": "59aqJJSj6-bh",
        "outputId": "9a9bfc7c-fc34-4e48-f1ef-b0fd69b53bf4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6325/6325 [43:31<00:00,  2.42it/s]\n",
            "100%|██████████| 1117/1117 [07:26<00:00,  2.50it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reducing Embedding Dimension\n"
      ],
      "metadata": {
        "id": "5UYOYK1fMwAM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=300)\n",
        "train_dataframe['reduced_embedding'] = pca.fit_transform(np.array([vec for vec in train_dataframe['embedding'].values])).tolist()\n",
        "test_dataframe['reduced_embedding'] = pca.transform(np.array([vec for vec in test_dataframe['embedding'].values])).tolist()"
      ],
      "metadata": {
        "id": "dw2k5pEQ8DEV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Processes (grouping, categorizing, ...)"
      ],
      "metadata": {
        "id": "Uc0Z0jNBNFHn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def final_dataset_generator(dataset):\n",
        "    wordshape = {'hard':[\"hard\", \"harder\", \"hardest\"],\n",
        "                 'interest': [\"interest\", \"interests\", \"interested\", \"interesting\"],\n",
        "                 'line': [\"line\", \"lines\"],\n",
        "                 'serve': [\"serve\", \"served\", \"serves\"]}\n",
        "    word_sense = {'hard': ['HARD1', 'HARD2', 'HARD3'],\n",
        "                  'interest': ['interest1', 'interest2', 'interest3', 'interest4', 'interest5', 'interest6'],\n",
        "                  'line': ['division', 'cord', 'phone', 'formation', 'product', 'text'],\n",
        "                  'serve': ['SERVE10', 'SERVE12']}\n",
        "    \n",
        "    output = {ws:[] for ws in wordshape}\n",
        "\n",
        "    le = LabelEncoder()\n",
        "    for ws in wordshape:\n",
        "        frames = []\n",
        "        for word in wordshape[ws]:\n",
        "            for sense in word_sense[ws]:\n",
        "                try:\n",
        "                    frames.append(dataset.groupby(['word', 'sense']).get_group((word,sense)))\n",
        "                except KeyError as e:\n",
        "                    pass\n",
        "        df = pd.concat(frames)\n",
        "        le.fit(df.sense)\n",
        "        x = np.array([np.array(vec) for vec in df.reduced_embedding.values])\n",
        "        y = le.transform(df.sense)\n",
        "        output[ws] = [x, y]\n",
        "    return output"
      ],
      "metadata": {
        "id": "8GQS62MYdyTS"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = final_dataset_generator(train_dataframe)\n",
        "test_dataset = final_dataset_generator(test_dataframe)"
      ],
      "metadata": {
        "id": "VDcCHq7pJE0T"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results"
      ],
      "metadata": {
        "id": "o2QypxwINmua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for word in train_dataset:\n",
        "    xtest, ytest = test_dataset[word]\n",
        "    xtrain, ytrain = train_dataset[word]\n",
        "    try:\n",
        "        cls = svm.SVC(kernel='linear').fit(xtrain, ytrain)\n",
        "\n",
        "        print()\n",
        "        print(f\"========== {word} ===========\")\n",
        "        print('On Train dataset')\n",
        "        ypred = cls.predict(xtrain)\n",
        "        acc = accuracy_score(ytrain, ypred)\n",
        "        f1 = f1_score(ytrain, ypred, average='macro')\n",
        "        print(f'Acc: {acc:.3f}')\n",
        "        print(f'F1: {f1:.3f}')\n",
        "\n",
        "        print('On Test dataset')\n",
        "        ypred = cls.predict(xtest)\n",
        "        acc = accuracy_score(ytest, ypred)\n",
        "        f1 = f1_score(ytest, ypred, average='macro')\n",
        "        print(f'Acc: {acc:.3f}')\n",
        "        print(f'F1: {f1:.3f}')\n",
        "\n",
        "    except ValueError:\n",
        "        print(\"SVM classification can't be done for word:\", word)"
      ],
      "metadata": {
        "id": "UzOXKyyRo-61",
        "outputId": "93fc8552-b161-48db-bcf8-00a53c9b8727",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM classification can't be done for word: hard\n",
            "\n",
            "========== interest ===========\n",
            "On Train dataset\n",
            "Acc: 1.000\n",
            "F1: 1.000\n",
            "On Test dataset\n",
            "Acc: 0.940\n",
            "F1: 0.822\n",
            "\n",
            "========== line ===========\n",
            "On Train dataset\n",
            "Acc: 1.000\n",
            "F1: 1.000\n",
            "On Test dataset\n",
            "Acc: 0.948\n",
            "F1: 0.947\n",
            "\n",
            "========== serve ===========\n",
            "On Train dataset\n",
            "Acc: 1.000\n",
            "F1: 1.000\n",
            "On Test dataset\n",
            "Acc: 1.000\n",
            "F1: 1.000\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "part1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}