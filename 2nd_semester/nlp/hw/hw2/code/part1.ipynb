{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><h3>HW2, Part1</h3></div>\n",
    "<div align=\"center\"><h5>Mohammadreza Ghofrani, 400131076</h5></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hazm import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.decomposition import PCA\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import Word2Vec, TfidfModel\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxHolder:\n",
    "    def __init__(self):\n",
    "        self._max = -np.inf\n",
    "        self._max_index = None\n",
    "\n",
    "    @property\n",
    "    def max(self):\n",
    "        return (self._max, self._max_index)\n",
    "\n",
    "    @max.setter\n",
    "    def max(self, value):\n",
    "        v = value[0]\n",
    "        v_idx = value[1]\n",
    "        if v > self._max:\n",
    "            self._max = v\n",
    "            self._max_index = v_idx\n",
    "\n",
    "    @max.deleter\n",
    "    def max(self):\n",
    "        del self._max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Word Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../data/train.csv')\n",
    "test_df = pd.read_csv('../data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here each article is transformed into a list of its tokens\n",
    "def tokenizer(text):\n",
    "    normalizer = Normalizer()\n",
    "    txt = normalizer.normalize(text)\n",
    "    return word_tokenize(txt)\n",
    "\n",
    "tokens_of_articles = list()\n",
    "\n",
    "for article in train_df['article']:\n",
    "    article_tokens = tokenizer(article)\n",
    "    tokens_of_articles.append(article_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = Word2Vec(sentences=tokens_of_articles, vector_size=300, min_count=0,\n",
    "                          window=5, workers=4, sg=1, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Document Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dct = Dictionary(tokens_of_articles)\n",
    "bow_corpus = [dct.doc2bow(line) for line in tokens_of_articles]\n",
    "tfidf_model = TfidfModel(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf_repr(doc):\n",
    "    doc_repr = 0\n",
    "    doc_weight_sum = 0\n",
    "    for word_id, tfidf_score in doc:\n",
    "        word = dct[word_id]\n",
    "        word2vec_score = word2vec_model.wv[word]\n",
    "        doc_repr += (tfidf_score * word2vec_score)\n",
    "        doc_weight_sum += tfidf_score\n",
    "    return doc_repr / doc_weight_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_representation_of_documents = list()\n",
    "for doc in tfidf_model[bow_corpus]:\n",
    "    tfidf_repr = get_tfidf_repr(doc)\n",
    "    word2vec_representation_of_documents.append(tfidf_repr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_articles = [TaggedDocument(art, [i]) for i, art in enumerate(tokens_of_articles)]\n",
    "doc2vec_model = Doc2Vec(tagged_articles, vector_size=300, window=5,\n",
    "                        min_count=0, workers=6, dm=0, epochs=7)\n",
    "doc2vec_model.build_vocab(tagged_articles)\n",
    "doc2vec_model.train(tagged_articles, total_examples=doc2vec_model.corpus_count,\n",
    "                    epochs=doc2vec_model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vec_representation_of_documents = list()\n",
    "for art_tokens in tokens_of_articles:\n",
    "    doc2vec_repr = doc2vec_model.infer_vector(art_tokens)\n",
    "    doc2vec_representation_of_documents.append(doc2vec_repr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Similar Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TF-IDF&word2vec\n",
      "\tmost similar document to Doc1 is Doc165 with similarity 0.9894\n",
      "\tmost similar document to Doc3 is Doc19 with similarity 0.9961\n",
      "\tmost similar document to Doc5 is Doc26 with similarity 0.9885\n",
      "\tmost similar document to Doc25 is Doc679 with similarity 1.0000\n",
      "\tmost similar document to Doc36 is Doc7 with similarity 0.9885\n"
     ]
    }
   ],
   "source": [
    "print('Using TF-IDF&word2vec')\n",
    "for ref_docid in ['Doc1', 'Doc3', 'Doc5', 'Doc25', 'Doc36']:\n",
    "    article = test_df[test_df['id'] == ref_docid]['article'].values[0]\n",
    "    ref_article_tokens = tokenizer(article)\n",
    "    ref_article_embedding = get_tfidf_repr(tfidf_model[dct.doc2bow(ref_article_tokens)])\n",
    "\n",
    "    max_holder = MaxHolder()\n",
    "    for docindex, _ in enumerate(tokens_of_articles):\n",
    "        cur_article_embedding = word2vec_representation_of_documents[docindex]\n",
    "        sim = cosine_similarity(ref_article_embedding[np.newaxis], cur_article_embedding[np.newaxis])\n",
    "        max_holder.max = (sim, docindex)\n",
    "\n",
    "    sim, docindex = max_holder.max\n",
    "    sim_docid = train_df.iloc[docindex][0]\n",
    "    print(f'\\tmost similar document to {ref_docid} is {sim_docid} with similarity {sim[0][0]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using doc2vec\n",
      "\tmost similar document to Doc1 is Doc33 with similarity 0.675\n",
      "\tmost similar document to Doc3 is Doc19 with similarity 0.861\n",
      "\tmost similar document to Doc5 is Doc0 with similarity 0.628\n",
      "\tmost similar document to Doc25 is Doc679 with similarity 0.996\n",
      "\tmost similar document to Doc36 is Doc406 with similarity 0.613\n"
     ]
    }
   ],
   "source": [
    "print('Using doc2vec')\n",
    "for ref_docid in ['Doc1', 'Doc3', 'Doc5', 'Doc25', 'Doc36']:\n",
    "    article = test_df[test_df['id'] == ref_docid]['article'].values[0]\n",
    "    ref_article_tokens = tokenizer(article)\n",
    "    ref_article_embedding = doc2vec_model.infer_vector(ref_article_tokens)\n",
    "\n",
    "    max_holder = MaxHolder()\n",
    "    for docindex, _ in enumerate(tokens_of_articles):\n",
    "        cur_article_embedding = doc2vec_representation_of_documents[docindex]\n",
    "        sim = cosine_similarity(ref_article_embedding[np.newaxis], cur_article_embedding[np.newaxis])\n",
    "        max_holder.max = (sim, docindex)\n",
    "\n",
    "    sim, docindex = max_holder.max\n",
    "    sim_docid = train_df.iloc[docindex][0]\n",
    "    print(f'\\tmost similar document to {ref_docid} is {sim_docid} with similarity {sim[0][0]:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Similar Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding most similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most similar words to تهران are:\n",
      "[('تبریز', 0.6092485785484314),\n",
      " ('اصفهان', 0.6041479110717773),\n",
      " ('مشهد', 0.5989333391189575)]\n",
      "\n",
      "most similar words to بهداشت are:\n",
      "[('باروری', 0.8025951385498047),\n",
      " ('مراقبتهای', 0.7638570666313171),\n",
      " ('بهورزان', 0.7540599703788757)]\n",
      "\n",
      "most similar words to دفاع are:\n",
      "[('مقدس', 0.703788697719574),\n",
      " ('ضدموشکی', 0.6654548645019531),\n",
      " ('پشتیبانی', 0.6579294800758362)]\n",
      "\n",
      "most similar words to رودخانه are:\n",
      "[('دریاچه', 0.8565274477005005),\n",
      " ('کارون', 0.8331038355827332),\n",
      " ('ارتفاعات', 0.8268086910247803)]\n",
      "\n",
      "most similar words to سرد are:\n",
      "[('زمستان', 0.7902213931083679),\n",
      " ('مرطوب', 0.7889065146446228),\n",
      " ('زمستان\\u200cهای', 0.7770519852638245)]\n",
      "\n",
      "most similar words to فرهنگ are:\n",
      "[('ارشاد', 0.7776681780815125),\n",
      " ('سمعی', 0.6865524649620056),\n",
      " ('هنر', 0.6761164665222168)]\n",
      "\n",
      "most similar words to استقلال are:\n",
      "[('پاس', 0.70670086145401),\n",
      " ('صنام', 0.6973098516464233),\n",
      " ('پیکان', 0.6805547475814819)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sim_word_embeddings = list()\n",
    "sim_word_embeddings_label = list()\n",
    "sim_word_embeddings_cluster = list()\n",
    "for i, w in enumerate(['تهران', 'بهداشت', 'دفاع', 'رودخانه', 'سرد', 'فرهنگ', 'استقلال']):\n",
    "    most_sim_words = word2vec_model.wv.most_similar(positive=[w], topn=3)\n",
    "\n",
    "    print(f'most similar words to {w} are:')\n",
    "    pprint(most_sim_words)\n",
    "    print()\n",
    "\n",
    "    for sim_word, _ in most_sim_words + [(w,1)]:\n",
    "        sim_word_embeddings.append(word2vec_model.wv.get_vector(sim_word))\n",
    "        sim_word_embeddings_label.append(sim_word)\n",
    "        sim_word_embeddings_cluster.append(i)\n",
    "\n",
    "sim_word_embeddings = np.array(sim_word_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing most similar words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_2d = PCA(n_components=2)\n",
    "sim_word_2d_embeddings = pca_2d.fit_transform(sim_word_embeddings)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=sim_word_2d_embeddings[:,0],\n",
    "    y=sim_word_2d_embeddings[:,1],\n",
    "    mode=\"markers+text\",\n",
    "    marker=dict(color=sim_word_embeddings_cluster),\n",
    "    name=\"Markers and Text\",\n",
    "    text=sim_word_embeddings_label,\n",
    "    textposition=\"bottom center\"\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=\"Visualization of word embeddings in 2D\",\n",
    "    title_x=0.5,\n",
    "    font=dict(\n",
    "        family=\"Vazir\",\n",
    "        size=13\n",
    "    ),\n",
    "    width=1000,\n",
    "    height=700\n",
    ")\n",
    "\n",
    "fig.write_image(\"../images/2d_embedding_similarity.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_3d = PCA(n_components=3)\n",
    "sim_word_2d_embeddings = pca_3d.fit_transform(sim_word_embeddings)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter3d(\n",
    "    x=sim_word_2d_embeddings[:,0],\n",
    "    y=sim_word_2d_embeddings[:,1],\n",
    "    z=sim_word_2d_embeddings[:,2],\n",
    "    mode=\"markers+text\",\n",
    "    marker=dict(color=sim_word_embeddings_cluster),\n",
    "    name=\"Markers and Text\",\n",
    "    text=sim_word_embeddings_label,\n",
    "    textposition=\"bottom center\"\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=\"Visualization of word embeddings in 3D\",\n",
    "    title_x=0.5,\n",
    "    font=dict(\n",
    "        family=\"Vazir\",\n",
    "        size=13\n",
    "    ),\n",
    "    width=800,\n",
    "    height=1000\n",
    ")\n",
    "\n",
    "fig.write_image(\"../images/3d_embedding_similarity.png\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7190ae378dd7298a835d7d91d57b4572e1a91bb8f4b6d673c87ae831e3ab4f27"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
