\documentclass{article}

\usepackage{amsmath}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{subcaption}
\usepackage{xepersian}

\settextfont{Sahel}
\setmathdigitfont{Yas}
\setlatintextfont{Times Newer Roman}

\title{\vspace{-4cm} \textbf{پاسخ تمرینات سری دوم درس یادگیری ماشین}}
\author{محمدرضا غفرانی  ۴۰۰۱۳۱۰۷۶}
\date{\today}

\begin{document}

\maketitle

\section*{سوال یک}

\subsection*{قسمت الف}

داده‌ها به طور کلی در سه ناحیه‌ای که در شکل \ref{districts-of-samples} نشان داده شده است، قرار دارند.
داده‌های موجود در ناحیه‌ ۱ با انتخاب مقادیر مختلف $k$ به درستی برچسب می‌خورد. داده‌های موجود در ناحیه‌ی
۲ علی‌رغم محدود بودن، تنها به ازای انتخاب مقادیر خاص $k$ به درستی برچسب می‌خورد. داده‌های موجود در ناحیه‌ ۳
دارای تعداد زیادی هستند و با انتخاب مقدار درست $k$ به درستی برچسب می‌خورند.

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.3]{images/q1/districts.png}
    \caption{تقسیم داده‌ها به سه ناحیه}
    \label{districts-of-samples}
\end{figure}

از آن جا که هدف ما کمینه کردن خطای \lr{LOOCV} است، بنابراین در درجه‌ اول سعی می‌کنیم داده‌های موجود در
ناحیه ۳ به درستی برچسب زده شود. در قدم بعدی سعی می‌کنیم داده‌ی موجود در ناحیه ۲ نیز به درستی برچسب بخورد.
در شکل \ref{distance-of-positive-data-relative-to-others} فاصله‌ی نسبی یکی از داده‌های موجود در ناحیه ۳
از سایر داده‌ها رسم شده است.

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.4]{images/q1/distances.png}
    \caption{داده‌ی مثبتی که انتخاب $k$ تاثیر زیادی در نتیجه‌ی آن دارد.}
    \label{distance-of-positive-data-relative-to-others}
\end{figure}

با توجه به شکل رابطه‌‌ی بین $d_i$ها به شکل زیر است. (فواصل مربوط به داده‌های مثبت با رنگ سبز
و فواصل مربوط به داده‌های منفی با رنگ قرمز مشخص شده است.)

$$\textcolor{red}{d_3} < \textcolor{green}{d_1} = \textcolor{green}{d_4} < \textcolor{red}{d_9} <
\textcolor{green}{d_2} < \textcolor{red}{d_5} < \textcolor{red}{d_8} < \textcolor{red}{d_7} < \textcolor{red}{d_6} $$

با توجه به فاصله‌های بالا برای آن‌ که این داده به درستی برچسب زده شود، باید

$$2 < k < 6$$

البته $k=2$ یا $k=6$ نیز به شرطی قابل قبول هستند که به برچسب مثبت اولویت بیشتری داده شود.
بنابراین اگر $2 \leq k \leq 6$ باشد، داده‌های موجود در ناحیه ۳ به درستی برچسب می‌خورد. برای آن که
داده‌ی موجود در ناحیه ۲ به درستی برچسب بخورد حداقل $k$ باید برابر 9 باشد. اما با انتخاب $k=9$
بسیاری از داده‌ها برچسب اشتباه برچسب زده می‌شوند. درنتیجه برای رسیدن به کمترین خطای \lr{LOOCV}
باید از درست‌ برچسب خوردن این داده صرف نظر کنیم، در نتیجه با انتخاب $2 \leq k \leq 6$ به کمترین
خطای \lr{LOOCV} می‌رسیم.

\subsection*{قسمت ب}

بر طبق یک قاعده‌ی سرانگشتی\LTRfootnote{Rule of Thumb} اگر تعداد $n$ داده آموزشی داشته باشیم، در این صورت
$\sqrt{n}$ یک مقدار خوب برای $k$ است. اما در حالت کلی روش عملی برای پیدا کردن مقدار بهینه $k$ وجود ندارد.
معمولا برای پیدا کردن مقدار بهینه $k$، مقادیر مختلفی را برای $k$ در نظر گرفته و با تکرار روش \lr{k-Fold Cross Validation}
مقداری از $k$ که به ازای آن خطای کمینه می‌شود را انتخاب می‌کنند.

\section*{سوال دو}

هر دو الگوریتم \lr{KNN} و درخت تصمیم از نوع الگوریتم‌های تمایزگر هستند چرا که با مقایسه‌ی داده‌ها با یکدیگر
سعی می‌کنند یک مرز تصمیم بین داده‌های از کلاس‌های مختلف پیدا کنند.

\section*{سوال سه}

\subsection*{قسمت الف}

داریم.

$$\sigma(a) = \frac{1}{1+e^{-a}}$$

بنابراین.

\begin{eqnarray*}
    \frac{d\sigma(a)}{da} = \frac{0-(-1)e^{-a}}{(1+e^{-a})^2} & = & \frac{e^{-a}}{(1+e^{-a})^2} \\
    & = & \frac{1}{1+e^{-a}} \times \frac{e^{-a}}{1+e^{-a}} \\
    & = & \frac{1}{1+e^{-a}} \times (1 - \frac{1}{1+e^{-a}}) \\
    & = & \sigma(a) (1-\sigma(a)) \\
\end{eqnarray*}

\subsection*{قسمت ب}

داریم.

\begin{eqnarray*}
    y^{(i)} = 1 & \Longrightarrow & f_{i1} = -\ln(p(c_1|x^{(i)})) = -\ln(\sigma(w^Tx^i)) = - \ln(\hat{y}^i) \\
    y^{(i)} = 0 & \Longrightarrow & f_{i2} = -\ln(p(c_0|x^{(i)})) = -\ln(1-\sigma(w^Tx^i)) = - \ln(1-\hat{y}^i)
\end{eqnarray*}
\begin{eqnarray*}
     \longrightarrow f_i & = & f_{i1} + f_{i2} \\
     & = & - y^i \ln(\hat{y}^i) +(1-y^i)\ln(1-\hat{y}^i)
\end{eqnarray*}
\begin{eqnarray*}
     f & = & \sum_{i=1}^{n} f_{i} \\
     & = & - \sum_{i=1}^{n} y^i \ln(\hat{y}^i) +(1-y^i)\ln(1-\hat{y}^i)
\end{eqnarray*}

\subsection*{قسمت ج}

با مشتق‌گیری از عبارت بالا خواهیم داشت.

\begin{eqnarray*}
    \frac{df}{dw} & = & \sum_{i=1}^{n} \frac{df_{i}}{dw} \\
    & = & - \sum_{i=1}^{n} \frac{df_{i1}}{dw} + \frac{df_{i2}}{dw}
\end{eqnarray*}
\begin{eqnarray*}
    \frac{df_{i1}}{dw} & = & \frac{d(-\ln(\hat{y}^i)))}{dw} = -(1-\hat{y}^i)x^i =  -(y^i - \hat{y}^i)x^i\\
    \frac{df_{i2}}{dw} & = & \frac{d(-\ln(1-y^i))}{dw} = \hat{y}^ix^i = -(0 - y^i)x^i = -(y^i - \hat{y}^i)x^i
\end{eqnarray*}
\begin{eqnarray*}
    \frac{df}{dw} = -\sum_{i=1}^{n} -(y^i - \hat{y}^i)x^i -(y^i - \hat{y}^i)x^i = \sum_{i=1}^{n} (y^i - \hat{y}^i)x^i
\end{eqnarray*}

\subsection*{قسمت د}

در این حالت از آن جا که چندین صفحه متمایز قادر به جداسازی کلاس‌ها از یکدیگر است بنابراین
الگوریتم \lr{logistic regression} نمی‌تواند یک جواب یکتا برای مسئله پیدا کند. در این حالت
در هر مرحله الگوریتم جواب خود را تغییر داده و به عبارتی دچار ناپایداری می‌شود.

\section*{سوال چهار}

\subsection*{قسمت الف}

با توجه به محاسبات زیر فرد \lr{X1} جنس مد نظر را خریداری می‌کند.

\begin{eqnarray*}
    P(\text{\lr{buy}}=\text{\lr{Yes}}) = \frac{9}{14} & \hspace{2cm} & P(\text{\lr{buy}}=\text{\lr{No}}) = \frac{5}{14}\\
    P(\text{\lr{age}}=\text{\lr{Youth}}) = \frac{2}{9} & \hspace{2cm} & P(\text{\lr{age}}=\text{\lr{Youth}}) = \frac{3}{5}\\
    P(\text{\lr{income}}=\text{\lr{High}}) = \frac{2}{9} & \hspace{2cm} & P(\text{\lr{income}}=\text{\lr{High}}) = \frac{2}{5}\\
    P(\text{\lr{student}}=\text{\lr{Yes}}) = \frac{6}{9} & \hspace{2cm} & P(\text{\lr{student}}=\text{\lr{Yes}}) = \frac{1}{5}\\
    P(\text{\lr{credit}}=\text{\lr{Fair}}) = \frac{6}{9} & \hspace{2cm} & P(\text{\lr{credit}}=\text{\lr{Fair}}) = \frac{2}{5}\\
\end{eqnarray*}

بنابراین

\begin{eqnarray*}
    P(\text{\lr{buy}}=\text{\lr{Yes}})\prod_{i} P(x_i|\text{\lr{buy}}=\text{\lr{Yes}}) & = &  \frac{9}{14} \times \frac{2}{9} \times \frac{2}{9} \times \frac{6}{9} \times \frac{6}{9} =  \frac{1296}{91854} \simeq 0.014\\
    P(\text{\lr{buy}}=\text{\lr{No}})\prod_{i} P(x_i|\text{\lr{buy}}=\text{\lr{No}})  & = & \frac{5}{14} \times \frac{3}{5} \times \frac{2}{5} \times \frac{1}{5} \times \frac{2}{5} = \frac{60}{8750} \simeq 0.006
\end{eqnarray*}

\begin{center}
\rule{0.5\linewidth}{0.1pt}
\end{center}

با توجه به محاسبات زیر فرد \lr{X2} جنس مورد نظر را خریداری نمی‌کند.

\begin{eqnarray*}
    P(\text{\lr{buy}}=\text{\lr{Yes}}) = \frac{9}{14} & \hspace{2cm} & P(\text{\lr{buy}}=\text{\lr{No}}) = \frac{5}{14}\\
    P(\text{\lr{age}}=\text{\lr{Senior}}) = \frac{3}{9} & \hspace{2cm} & P(\text{\lr{age}}=\text{\lr{Senior}}) = \frac{2}{5}\\
    P(\text{\lr{income}}=\text{\lr{Low}}) = \frac{3}{9} & \hspace{2cm} & P(\text{\lr{income}}=\text{\lr{Low}}) = \frac{1}{5}\\
    P(\text{\lr{student}}=\text{\lr{No}}) = \frac{3}{9} & \hspace{2cm} & P(\text{\lr{student}}=\text{\lr{No}}) = \frac{4}{5}\\
    P(\text{\lr{credit}}=\text{\lr{Excellent}}) = \frac{3}{9} & \hspace{2cm} & P(\text{\lr{credit}}=\text{\lr{Excellent}}) = \frac{3}{5}\\
\end{eqnarray*}

بنابراین

\begin{eqnarray*}
    P(\text{\lr{buy}}=\text{\lr{Yes}})\prod_{i} P(x_i|\text{\lr{buy}}=\text{\lr{Yes}}) & = &  \frac{9}{14} \times \frac{3}{9} \times \frac{3}{9} \times \frac{3}{9} \times \frac{3}{9} =  \frac{324}{91854} \simeq 0.007\\
    P(\text{\lr{buy}}=\text{\lr{No}})\prod_{i} P(x_i|\text{\lr{buy}}=\text{\lr{No}})  & = & \frac{5}{14} \times \frac{2}{5} \times \frac{1}{5} \times \frac{4}{5} \times \frac{3}{5} = \frac{120}{8750} \simeq 0.013
\end{eqnarray*}

\begin{center}
\rule{0.5\linewidth}{0.1pt}
\end{center}

با توجه به محاسبات زیر فرد \lr{X3} جنس مورد نظر را خریداری می‌کند.

\begin{eqnarray*}
    P(\text{\lr{buy}}=\text{\lr{Yes}}) = \frac{9}{14} & \hspace{2cm} & P(\text{\lr{buy}}=\text{\lr{No}}) = \frac{5}{14}\\
    P(\text{\lr{age}}=\text{\lr{Middle}}) = \frac{4}{9} & \hspace{2cm} & P(\text{\lr{age}}=\text{\lr{Middle}}) = 0 \\
    P(\text{\lr{income}}=\text{\lr{Medium}}) = \frac{4}{9} & \hspace{2cm} & P(\text{\lr{income}}=\text{\lr{Medium}}) = \frac{2}{5}\\
    P(\text{\lr{student}}=\text{\lr{No}}) = \frac{3}{9} & \hspace{2cm} & P(\text{\lr{student}}=\text{\lr{No}}) = \frac{4}{5}\\
    P(\text{\lr{credit}}=\text{\lr{Fair}}) = \frac{6}{9} & \hspace{2cm} & P(\text{\lr{credit}}=\text{\lr{Fair}}) = \frac{2}{5}\\
\end{eqnarray*}

بنابراین

\begin{eqnarray*}
    P(\text{\lr{buy}}=\text{\lr{Yes}})\prod_{i} P(x_i|\text{\lr{buy}}=\text{\lr{Yes}}) & = &  \frac{9}{14} \times \frac{4}{9} \times \frac{4}{9} \times \frac{3}{9} \times \frac{6}{9} = \frac{2592}{91854} \simeq 0.028\\
    P(\text{\lr{buy}}=\text{\lr{No}})\prod_{i} P(x_i|\text{\lr{buy}}=\text{\lr{No}})  & = & \frac{5}{14} \times 0 \times \frac{2}{5} \times \frac{4}{5} \times \frac{2}{5} = 0
\end{eqnarray*}

\subsection*{قسمت ب}

ابتدا متغیری که بیشتری بهره یادگیری را دارد محاسبه کرده و در ریشه قرار می‌دهیم.

\begin{eqnarray*}
    E(S) & = & -(p_{+} \log_2(p_{+}) + p_{-} \log_2(p_{-})) \\
    & = & - (\frac{9}{14} \log_2{\frac{9}{14}} + \frac{5}{14} \log_2(\frac{5}{14})) \\
    & = & 0.819
\end{eqnarray*}
\begin{eqnarray*}
    IG(\text{\lr{Age}}) & = & E(S) - \sum_{v \in {\text{\lr{Youth, Middle, Senior}}}} \frac{|S_v|}{S} E(S_v) \\
    & = & 0.819 - (\frac{5}{14} E(S_{\text{\lr{Youth}}}) + \frac{4}{14} E(S_{\text{\lr{Middle}}}) + \frac{5}{14} E(S_{\text{\lr{Senior}}}))\\
    & \Longrightarrow & E(S_{\text{\lr{Youth}}}) = -(\frac{3}{5} \log_2(\frac{3}{5}) + \frac{2}{5} \log_2(\frac{2}{5})) = 0.970 \\
    & \Longrightarrow & E(S_{\text{\lr{Middle}}}) = -(\frac{0}{4} \log_2(\frac{0}{4}) + \frac{4}{4} \log_2(\frac{4}{4})) = 0 \\
    & \Longrightarrow & E(S_{\text{\lr{Senior}}}) = -(\frac{2}{5} \log_2(\frac{2}{5}) + \frac{3}{5} \log_2(\frac{3}{5})) = 0.970 \\
    & = & 0.819 - (\frac{5}{14} \times 0.970 + \frac{4}{14} \times 0 + \frac{5}{14} \times 0.970) \\
    IG(\text{\lr{Age}}) & = & 0.127
\end{eqnarray*}
\begin{eqnarray*}
    IG(\text{\lr{Income}}) & = & E(S) - \sum_{v \in {\text{\lr{Low, Medium, High}}}} \frac{|S_v|}{S} E(S_v) \\
    & = & 0.819 - (\frac{4}{14} E(S_{\text{\lr{Low}}}) + \frac{6}{14} E(S_{\text{\lr{Medium}}}) + \frac{4}{14} E(S_{\text{\lr{High}}}))\\
    & \Longrightarrow & E(S_{\text{\lr{Low}}}) = -(\frac{1}{4} \log_2(\frac{1}{4}) + \frac{3}{4} \log_2(\frac{3}{4})) = 0.811 \\
    & \Longrightarrow & E(S_{\text{\lr{Medium}}}) = -(\frac{2}{6} \log_2(\frac{2}{6}) + \frac{4}{6} \log_2(\frac{4}{6})) = 0.918 \\
    & \Longrightarrow & E(S_{\text{\lr{High}}}) = -(\frac{2}{4} \log_2(\frac{2}{4}) + \frac{2}{4} \log_2(\frac{2}{4})) = 1 \\
    & = & 0.819 - (\frac{4}{14} \times 0.811 + \frac{6}{14} \times 0.918 + \frac{4}{14} \times 1 )\\
    IG(\text{\lr{Income}}) & = & -0.091
\end{eqnarray*}
\begin{eqnarray*}
    IG(\text{\lr{Student}}) & = & E(S) - \sum_{v \in {\text{\lr{No, Yes}}}} \frac{|S_v|}{S} E(S_v) \\
    & = & 0.819 - (\frac{7}{14} E(S_{\text{\lr{No}}}) + \frac{7}{14} E(S_{\text{\lr{Yes}}})))\\
    & \Longrightarrow & E(S_{\text{\lr{No}}}) = -(\frac{4}{7} \log_2(\frac{4}{7}) + \frac{3}{7} \log_2(\frac{3}{7})) = 0.985 \\
    & \Longrightarrow & E(S_{\text{\lr{Yes}}}) = -(\frac{1}{7} \log_2(\frac{1}{7}) + \frac{6}{7} \log_2(\frac{6}{7})) = 0.591 \\
    & = & 0.819 - (\frac{7}{14} \times 0.985 + \frac{7}{14} \times 0.591 )\\
    IG(\text{\lr{Student}}) & = & 0.03
\end{eqnarray*}
\begin{eqnarray*}
    IG(\text{\lr{Credit}}) & = & E(S) - \sum_{v \in {\text{\lr{Fair, Excellent}}}} \frac{|S_v|}{S} E(S_v) \\
    & = & 0.819 - (\frac{8}{14} E(S_{\text{\lr{Fair}}}) + \frac{6}{14} E(S_{\text{\lr{Excellent}}})))\\
    & \Longrightarrow & E(S_{\text{\lr{Fair}}}) = -(\frac{2}{8} \log_2(\frac{2}{8}) + \frac{6}{8} \log_2(\frac{6}{8})) = 0.811 \\
    & \Longrightarrow & E(S_{\text{\lr{Excellent}}}) = -(\frac{3}{6} \log_2(\frac{3}{6}) + \frac{3}{6} \log_2(\frac{3}{6})) = 1 \\
    & = & 0.819 - (\frac{8}{14} \times 0.811 + \frac{6}{14} \times 1 )\\
    IG(\text{\lr{Credit}}) & = & -0.072
\end{eqnarray*}

با توجه به این محاسبات بهترین ویژگی برای ریشه \lr{Age} است. ادامه روال را نیز می‌توان با محاسبه‌
بهره اطلاعاتی پیش رفت، اما برای ساده‌تر شدن، سعی می‌کنیم با بحث روی مقادیر جدول مسئله را حل کنیم.
در حال حاضر درخت ما دارای سه گره است. در ادامه وضعیت جدول به ازای هر یک از این گره‌ها را بررسی می‌کنیم.

\begin{enumerate}
    \item شاخه‌ \lr{Youth}: با انتخاب \lr{Age = Youth} جدول به شکل \ref{youth-branch} در می‌آید. همان‌طور که مشاهده
    می‌شود در این جدول با استفاده از ویژگی \lr{Student} جدول را به دو شاخه کاملا خالص تبدیل کرد در صورتی که
    چنین اتفاقی با انتخاب سایر ویژگی‌ها رخ نمی‌دهد.
    \begin{figure}[h]
        \centering
        \includegraphics[scale=0.5]{images/q4/youth.png}
        \caption{در گره \lr{Youth} جدول به این شکل در می‌آید.}
        \label{youth-branch}
    \end{figure}
    \item شاخه‌ \lr{Middle}: با توجه به آن که به ازای انتخاب \lr{Age=Middle} به یک شاخه کاملا خالص می‌رسیم،
    بنابراین در این شاخه بحثی برای ادامه دسته‌بندی باقی نمی‌ماند. (شکل \ref{middle-branch})
    \begin{figure}[h]
        \centering
        \includegraphics[scale=0.5]{images/q4/middle.png}
        \caption{در گره \lr{Middle} جدول به این شکل در می‌آید.}
        \label{middle-branch}
    \end{figure}
    \item شاخه‌ \lr{Senior}: با انتخاب \lr{Age = Senior} جدول به شکل \ref{senior-branch} در می‌آید. همان‌طور که مشاهده
    می‌شود در این حالت نیز مشابه اتفاقی که در شاخه \lr{Youth} افتاد، رخ می‌دهد.
    \begin{figure}[h]
        \centering
        \includegraphics[scale=0.5]{images/q4/senior.png}
        \caption{در گره \lr{Senior} جدول به این شکل در می‌آید.}
        \label{senior-branch}
    \end{figure}
\end{enumerate}

با توجه به توضیحات درخت تصمیم به شکل \ref{q4-final-decision-tree} حاصل می‌شود.

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.5]{images/q4/decision-tree.png}
    \caption{درخت تصمیم سوال چهار قسمت ب}
    \label{q4-final-decision-tree}
\end{figure}

\section*{سوال پنج}

\subsection*{قسمت الف}

برای انجام دسته‌بندی از الگوریتم \lr{REPTree} استفاده می‌کنیم.
خروجی‌های این کد در شکل \ref{question5-parta} آورده شده است.

\begin{figure}[h]
    \begin{subfigure}{0.48\linewidth}
        \includegraphics[width=\linewidth]{images/q5/tree_visualization.png}
        \caption{درخت تصمیم‌گیر تولید شده}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.48\linewidth}
        \includegraphics[width=\linewidth]{images/q5/weka_tree_generator.png}
        \caption{پنجره مربوط به درخت تصمیم‌گیر در نرم‌افزار \lr{Weka}}
    \end{subfigure}
    \caption{شکل سوال پنج قسمت الف}
    \label{question5-parta}
\end{figure}

\subsection*{قسمت ب}

در پنجره نرم‌افزار \lr{Weka}ماتریس در هم ریختگی گزارش شده است. تصویر این ماتریس در شکل
\ref{q5-confusion-matrix} آورده شده است.

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.5]{images/q5/confusion_matrix.png}
    \caption{درخت تصمیم سوال پنج قسمت ب}
    \label{q5-confusion-matrix}
\end{figure}

\subsection*{قسمت ج}

درخت تصمیم به صورت موجود در شکل \ref{question5-partc} حاصل می‌شود.

\begin{figure}[h]
    \begin{subfigure}{0.48\linewidth}
        \includegraphics[width=\linewidth]{images/q5/tree_visualization_no_pruning.png}
        \caption{درخت تصمیم‌گیر تولید شده در حالت بدون هرس}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.48\linewidth}
        \includegraphics[width=\linewidth]{images/q5/weka_tree_generator_no_pruning.png}
        \caption{پنجره مربوط به درخت تصمیم‌گیر بدون انجام عملیات هرس در نرم‌افزار \lr{Weka}}
    \end{subfigure}
    \caption{شکل سوال پنج قسمت ج}
    \label{question5-partc}
\end{figure}

\subsection*{قسمت د}

همان طور که انتظار می‌رفت هنگامی که درخت هرس نمی‌شود، درخت بزرگ‌تری تولید می‌شود. در حالت هرس کردن
با وجود آن که نتیجه روی داده‌های آموزشی اندکی افت پیدا می‌کند اما درخت تولید شده عملکرد بهتری
روی داده‌های تست دارد. درخت‌های تصمیم مربوط به هر یک از این حالات در شکل \ref{question5-parta}
و \ref{question5-partc} رسم شده است.

\section*{سوال‌های پیاده‌سازی}

\section*{سوال یک}

در این سوال داده‌ها با نسبت ۸۰ به ۲۰ جدا شده‌اند یعنی ۸۰ درصد داده‌ها برای آموزش و ۲۰ درصد
برای تست مدل در نظر گرفته شده است.

\subsection*{قسمت الف}

نتایج مربوط به داده‌های تست و آموزش در جدول \ref{implementation-q1-parta-metrics}
آورده شده است.

\begin{latin}
\begin{table}[h]
    \centering
    \caption{\rl{نتایج ارزیابی مدل بر روی داده‌های تست و آموزش، سوال یک قسمت یک}}
    \begin{tabular}{c|c|c|c|c|c|c|c}
        \multicolumn{4}{c|}{Train} & \multicolumn{4}{|c}{Test} \\
        \hline
        Precision & Recall & F1 & Accuracy & Precision & Recall & F1 & Accuracy \\
        \hline
        0.855 & 0.876 & 0.865 & 0.86 & 0.845 & 0.869 & 0.857 & 0.848
    \end{tabular}
    \label{implementation-q1-parta-metrics}
\end{table}
\end{latin}

\subsection*{قسمت ب}

نتایج مربوط به داده‌های تست و آموزش در جدول \ref{implementation-q1-partb-metrics}
آورده شده است.

\begin{latin}
\begin{table}[h]
    \centering
    \caption{\rl{نتایج ارزیابی مدل بر روی داده‌های تست و آموزش، سوال یک قسمت دو}}
    \begin{tabular}{c|c|c|c|c|c|c|c}
        \multicolumn{4}{c|}{Train} & \multicolumn{4}{|c}{Test} \\
        \hline
        Precision & Recall & F1 & Accuracy & Precision & Recall & F1 & Accuracy \\
        \hline
        0.824 & 0.885 & 0.854 & 0.845 & 0.807 & 0.867 & 0.836 & 0.824
    \end{tabular}
    \label{implementation-q1-partb-metrics}
\end{table}
\end{latin}

\subsection*{قسمت ج}

نتایج مربوط به داده‌های تست و آموزش در جدول \ref{implementation-q1-partc-chol-metrics} با حذف ستون
\lr{chol} آورده شده است. با مقایسه نتایج این جدول با قسمت ب مشاهده می‌شود که
نتایج بر روی داده‌های آموزشی اندکی بهتر شده است اما
حتی برخی از معیار‌های مربوط به داده‌های تست اندکی افت کرده است.

\begin{latin}
\begin{table}[h]
    \centering
    \caption{\rl{نتایج ارزیابی مدل بر روی داده‌های تست و آموزش، سوال یک قسمت ج}}
    \begin{tabular}{c|c|c|c|c|c|c|c}
        \multicolumn{4}{c|}{Train} & \multicolumn{4}{|c}{Test} \\
        \hline
        Precision & Recall & F1 & Accuracy & Precision & Recall & F1 & Accuracy \\
        \hline
        0.853 & 0.871 & 0.862 & 0.857 & 0.842 & 0.858 & 0.85 & 0.843
    \end{tabular}
    \label{implementation-q1-partc-chol-metrics}
\end{table}
\end{latin}

نتایج مربوط به داده‌های تست و آموزش در جدول \ref{implementation-q1-partc-oldpeak-metrics} با حذف ستون
\lr{oldpeak} آورده شده است. همان‌طور که مشاهده می‌شود نتایج بر روی داده‌های آموزشی تقریبا ثابت
باقی مانده است اما این نتایج بر روی داده‌های تست افت داشته است. بنابراین به نظر می‌رسد حذف
ویژگی \lr{oldpeak} نتایج حاصل شده را کم‌تر تحت تاثیر قرار دهد.

\begin{latin}
\begin{table}[h]
    \centering
    \caption{\rl{نتایج ارزیابی مدل بر روی داده‌های تست و آموزش، سوال یک قسمت ج}}
    \begin{tabular}{c|c|c|c|c|c|c|c}
        \multicolumn{4}{c|}{Train} & \multicolumn{4}{|c}{Test} \\
        \hline
        Precision & Recall & F1 & Accuracy & Precision & Recall & F1 & Accuracy \\
        \hline
        0.858 & 0.85 & 0.854 & 0.851 & 0.833 & 0.801 & 0.817 & 0.814
    \end{tabular}
    \label{implementation-q1-partc-oldpeak-metrics}
\end{table}
\end{latin}

\section*{سوال دو}

\subsection*{قسمت الف}

مقادیر نامعلوم  موجود در داده‌های آموزش و تست در شکل \ref{q2-missing-values} آمده است.

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.3]{images/implementation/q2/missing_values.png}
    \caption{درصد و تعداد مقادیر نامعلوم در هر ستون}
    \label{q2-missing-values}
\end{figure}

بر اساس این شکل پیش‌پردازش‌های زیر را برای حذف مقادیر نامعلوم انجام می‌دهیم.

\begin{enumerate}
    \item حذف ستون \lr{Cabin} در دادگان تست و آموزش. این ستون مقادیر نامعلوم زیادی دارد، اما
    به نظر می‌رسد که در مدل نهایی اثرگذار نباشد.
    \item جایگذاری میانگین مقادیر معلوم برای ستون‌های \lr{Age} و \lr{Fare}.
    \item جایگذاری مد داده‌های معلوم برای ستون \lr{Embarked}.
\end{enumerate}

بعلاوه موارد بالا ستون‌های \lr{PassengerID}، \lr{Name} و \lr{Ticket} نیز حذف می‌شود.
علت حذف کردن آن‌ها این است که این ستون‌ها بیشتر جنبه شناسه دارند و در نتیجه دخیل کردن آن‌ها در
ساخت درخت، باعث می‌شود درخت ساخته شده نتواند به خوبی روی داده تست جوابگو باشد و به نوعی \lr{overfit}
شود.

\subsection*{قسمت ب و ج}

برای این قسمت ابتدا درخت تصمیم را با استفاده از داده‌های آموزش ساخته و سپس بر روی داده‌های آموزش و
تست ارزیابی می‌کنیم. نتایج درخت تصمیم بر روی داده‌های آموزشی در فایل \lr{train\_predictions.csv}
و با استفاده از داده‌های تست در فایل \lr{test\_predictions.csv} ذخیره شده است.

\section*{سوال سه}

\subsection*{قسمت الف}

برای یافتن بهترین مقدار $k$ تنها مقادیر فردی را که در بازه $[1,30]$ قرار دارند را بررسی می‌کنیم.
دقت‌های حاصل شده در شکل \ref{implementation-question1-parta} مشاهده می‌شود. همان‌طور که مشاهده می‌شود،
به ازای $k=1$ بهترین دقت حاصل شده است.


\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{images/implementation/q1/parta.png}
    \caption{دقت‌های حاصل شده به ازای مقادیر مختلف $k$}
    \label{implementation-question1-parta}
\end{figure}

\subsection*{قسمت ب}

جدول درهم‌ریختگی این دسته‌بند در شکل \ref{implementation-question1-partb} آورده شده است. در این شکل جدول‌های متناظر
داده‌های آموزش و تست نشان داده شده است. با توجه به این جداول دقت در داده‌ی آموزشی
برابر $100$ درصد و در داده‌های تست برابر $96.67$ درصد است.

\begin{figure}[h]
    \begin{subfigure}{0.48\linewidth}
        \includegraphics[width=\linewidth]{images/implementation/q1/train_confusion_matrix.png}
        \caption{جدول درهم‌ریختگی داده‌های آموزش}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.48\linewidth}
        \includegraphics[width=\linewidth]{images/implementation/q1/test_confusion_matrix.png}
        \caption{جدول درهم‌ریختگی داده‌های تست}
    \end{subfigure}
    \caption{شکل سوال پنج قسمت ج}
    \label{implementation-question1-partb}
\end{figure}

\end{document}
